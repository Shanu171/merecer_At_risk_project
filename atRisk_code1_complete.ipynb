{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f9f8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f66625d0",
   "metadata": {},
   "source": [
    "## Complete Step-by-Step Solution: At-Risk Member Prediction & Causal Analytics\n",
    "-Problem Statement\n",
    "-Predict members who will make high claims OR develop high-risk conditions in the future (3-12 months   ahead), and identify root causes for actionable interventions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21034a6d",
   "metadata": {},
   "source": [
    "ðŸ“‹ STEP-BY-STEP SOLUTION ROADMAP\n",
    "-- PHASE 1: DATA PREPARATION & UNDERSTANDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c3d4fa",
   "metadata": {},
   "source": [
    "STEP 1: Define \"At-Risk\" Members Clearly\n",
    "1.1 Business Definition Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f934c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what constitutes \"at-risk\" - get stakeholder agreement\n",
    "\n",
    "AT_RISK_CRITERIA = {\n",
    "    'high_claim': {\n",
    "        'threshold_type': 'percentile',  # or absolute amount\n",
    "        'threshold_value': 90,  # 90th percentile\n",
    "        'time_window': '6_months',  # future prediction window\n",
    "        'min_amount': 1000  # Â£5,000 minimum\n",
    "    },\n",
    "    'high_risk_condition': {\n",
    "        'conditions': [\n",
    "            'Cancer',\n",
    "            'Cardiovascular Disease', \n",
    "            'Chronic Kidney Disease',\n",
    "            'Diabetes Complications',\n",
    "            'Mental Health Crisis',\n",
    "            'Musculoskeletal Chronic'\n",
    "        ],\n",
    "        'severity_threshold': 'requires_ongoing_treatment'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f429de",
   "metadata": {},
   "source": [
    "## 1.2 Create Ground Truth Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05012a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6487f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_df = pd.read_csv(\"uk_pmi_claims_200k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89253dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "membership_df = pd.read_csv(\"uk_pmi_membership_120k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ff1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Distribution:\n",
      "is_at_risk\n",
      "0    0.980267\n",
      "1    0.019733\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "At-risk rate: 1.97%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def create_target_labels(claims_df, membership_df, observation_date, prediction_window_months=6):\n",
    "    \"\"\"\n",
    "    Create target variables for at-risk prediction\n",
    "    \n",
    "    Parameters:\n",
    "    - observation_date: Date at which we make prediction (e.g., '2025-01-01')\n",
    "    - prediction_window_months: How far ahead to predict (default 6 months)\n",
    "    \"\"\"\n",
    "    \n",
    "    observation_date = pd.to_datetime(observation_date)\n",
    "    future_end_date = observation_date + timedelta(days=30*prediction_window_months)\n",
    "    \n",
    "    # Target 1: High Claim in Future Window\n",
    "    future_claims = claims_df[\n",
    "        (pd.to_datetime(claims_df['Paid Date']) > observation_date) &\n",
    "        (pd.to_datetime(claims_df['Paid Date']) <= future_end_date)\n",
    "    ]\n",
    "    \n",
    "    future_claim_amounts = future_claims.groupby('Claimant Unique ID')['Claim Amount'].sum()\n",
    "    high_claim_threshold = future_claim_amounts.quantile(0.90)\n",
    "    \n",
    "    target_df = membership_df[['Unique ID']].copy()\n",
    "    target_df['future_total_claims'] = target_df['Unique ID'].map(future_claim_amounts).fillna(0)\n",
    "    target_df['will_make_high_claim'] = (target_df['future_total_claims'] > high_claim_threshold).astype(int)\n",
    "    \n",
    "    # Target 2: High-Risk Condition Development\n",
    "    high_risk_conditions = [\n",
    "        'Cancer', 'Cardiovascular', 'Chronic Kidney', \n",
    "        'Diabetes', 'Mental Health', 'Chronic Pain'\n",
    "    ]\n",
    "    \n",
    "    future_conditions = future_claims.groupby('Claimant Unique ID')['Condition Category'].apply(\n",
    "        lambda x: any(condition in str(x).upper() for condition in [c.upper() for c in high_risk_conditions])\n",
    "    )\n",
    "    \n",
    "    target_df['will_develop_high_risk_condition'] = target_df['Unique ID'].map(future_conditions).fillna(0).astype(int)\n",
    "    \n",
    "    # Combined Target: At-Risk Member (either high claim OR high-risk condition)\n",
    "    target_df['is_at_risk'] = (\n",
    "        (target_df['will_make_high_claim'] == 1) | \n",
    "        (target_df['will_develop_high_risk_condition'] == 1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Additional: Risk severity score (0-100)\n",
    "    target_df['risk_severity_score'] = (\n",
    "        0.6 * target_df['will_make_high_claim'] + \n",
    "        0.4 * target_df['will_develop_high_risk_condition']\n",
    "    ) * 100\n",
    "    \n",
    "    return target_df\n",
    "\n",
    "# Execute\n",
    "observation_date = '2024-06-01'  # Use as your \"present\" for prediction\n",
    "targets = create_target_labels(claims_df, membership_df, observation_date)\n",
    "\n",
    "print(\"Target Distribution:\")\n",
    "print(targets['is_at_risk'].value_counts(normalize=True))\n",
    "print(f\"\\nAt-risk rate: {targets['is_at_risk'].mean()*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4ef28e",
   "metadata": {},
   "source": [
    "## STEP 2: Data Integration & Temporal Alignment\n",
    "#### 2.1 Create Point-in-Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86340d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical claims: 148054 records\n",
      "Date range: 2020-01-23 to 2024-05-31\n"
     ]
    }
   ],
   "source": [
    "def create_point_in_time_dataset(claims_df, membership_df, observation_date, lookback_months=24):\n",
    "    \"\"\"\n",
    "    Create features using ONLY data available before observation_date\n",
    "    This prevents data leakage\n",
    "    \"\"\"\n",
    "    \n",
    "    observation_date = pd.to_datetime(observation_date)\n",
    "    lookback_start = observation_date - timedelta(days=30*lookback_months)\n",
    "    \n",
    "    # Filter historical claims (before observation date)\n",
    "    historical_claims = claims_df[\n",
    "        pd.to_datetime(claims_df['Paid Date']) < observation_date\n",
    "    ].copy()\n",
    "    \n",
    "    # Filter to lookback window\n",
    "    historical_claims = historical_claims[\n",
    "        pd.to_datetime(historical_claims['Paid Date']) >= lookback_start\n",
    "    ]\n",
    "    \n",
    "    print(f\"Historical claims: {len(historical_claims)} records\")\n",
    "    print(f\"Date range: {historical_claims['Paid Date'].min()} to {historical_claims['Paid Date'].max()}\")\n",
    "    \n",
    "    return historical_claims, membership_df\n",
    "\n",
    "# Execute\n",
    "observation_date = '2024-06-01'\n",
    "historical_claims, membership = create_point_in_time_dataset(\n",
    "    claims_df, membership_df, observation_date, lookback_months=60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1171ff",
   "metadata": {},
   "source": [
    "#### 2.2 Join Tables with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe2ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Quality Checks ===\n",
      "Duplicate Claim IDs: 0\n",
      "Duplicate Member IDs: 0\n",
      "\n",
      "Join Statistics:\n",
      "_merge\n",
      "both          148054\n",
      "left_only      87143\n",
      "right_only         0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Members with no claims in period: 87143\n"
     ]
    }
   ],
   "source": [
    "def join_claims_membership(claims_df, membership_df):\n",
    "    \"\"\"\n",
    "    Join claims and membership with data quality checks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check for duplicates\n",
    "    print(\"=== Data Quality Checks ===\")\n",
    "    print(f\"Duplicate Claim IDs: {claims_df['Claim ID'].duplicated().sum()}\")\n",
    "    print(f\"Duplicate Member IDs: {membership_df['Unique ID'].duplicated().sum()}\")\n",
    "    \n",
    "    # Join\n",
    "    df = membership_df.merge(\n",
    "        claims_df,\n",
    "        left_on='Unique ID',\n",
    "        right_on='Claimant Unique ID',\n",
    "        how='left',\n",
    "        indicator=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nJoin Statistics:\")\n",
    "    print(df['_merge'].value_counts())\n",
    "    \n",
    "    # Identify members with no claims\n",
    "    members_no_claims = df[df['_merge'] == 'left_only']['Unique ID'].nunique()\n",
    "    print(f\"\\nMembers with no claims in period: {members_no_claims}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = join_claims_membership(historical_claims, membership)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6808c55c",
   "metadata": {},
   "source": [
    "### STEP 3: Comprehensive Feature Engineering\n",
    "#### 3.1 Demographic & Membership Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c71c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Client Name_y', 'Client Identifier_y',\n",
    "       'Scheme Category/ Section Name_y',\n",
    "       'Scheme Category/ Section Name Identifier_y', 'Status of Member_y','Unique Member Reference_y', 'Contract Start Date_y',\n",
    "       'Contract End Date_y'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8348e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [col[:-2] if col.endswith('_x') else col for col in df.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4779f37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Client Name', 'Client Identifier', 'Scheme Category/ Section Name',\n",
       "       'Scheme Category/ Section Name Identifier', 'Unique Member Reference',\n",
       "       'Unique ID', 'Status of Member', 'Status of Registration',\n",
       "       'Year of Birth', 'Gender', 'Short Post Code of Member',\n",
       "       'Contract Start Date', 'Contract End Date', 'Original Date of Joining',\n",
       "       'Scheme Policy Joining Date', 'Registration Status', 'Lapse Date',\n",
       "       'Claimant Unique ID', 'Claimant Year of Birth', 'Claimant Gender',\n",
       "       'Short Post Code', 'Claim ID', 'Incurred Date', 'Paid Date',\n",
       "       'Condition Code', 'Impairment Code', 'Condition Category',\n",
       "       'Treatment Type', 'Claim Type', 'Ancillary Service Type',\n",
       "       'Treatment Location', 'Provider Type', 'Admission Date',\n",
       "       'Discharge Date', 'Calculate Length of Service', 'Claim Amount',\n",
       "       'Amount Paid', '_merge'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e49cc9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features created: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>Year of Birth</th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "      <th>gender_Female</th>\n",
       "      <th>gender_Male</th>\n",
       "      <th>gender_Other</th>\n",
       "      <th>Original Date of Joining</th>\n",
       "      <th>membership_tenure_days</th>\n",
       "      <th>membership_tenure_years</th>\n",
       "      <th>Registration Status</th>\n",
       "      <th>is_active_member</th>\n",
       "      <th>Scheme Category/ Section Name</th>\n",
       "      <th>contract_duration_days</th>\n",
       "      <th>Lapse Date</th>\n",
       "      <th>has_lapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MEM00000001-01</td>\n",
       "      <td>1968</td>\n",
       "      <td>56</td>\n",
       "      <td>55-64</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-09-05</td>\n",
       "      <td>2096</td>\n",
       "      <td>5.738535</td>\n",
       "      <td>Active</td>\n",
       "      <td>1</td>\n",
       "      <td>Advanced Cover</td>\n",
       "      <td>365</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MEM00000001-02</td>\n",
       "      <td>1967</td>\n",
       "      <td>57</td>\n",
       "      <td>55-64</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>1832</td>\n",
       "      <td>5.015743</td>\n",
       "      <td>Active</td>\n",
       "      <td>1</td>\n",
       "      <td>Executive Cover</td>\n",
       "      <td>365</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEM00000002-01</td>\n",
       "      <td>1976</td>\n",
       "      <td>48</td>\n",
       "      <td>45-54</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-11</td>\n",
       "      <td>1359</td>\n",
       "      <td>3.720739</td>\n",
       "      <td>Active</td>\n",
       "      <td>1</td>\n",
       "      <td>Premier Cover</td>\n",
       "      <td>365</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MEM00000002-03</td>\n",
       "      <td>2005</td>\n",
       "      <td>19</td>\n",
       "      <td>&lt;25</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2019-07-23</td>\n",
       "      <td>1775</td>\n",
       "      <td>4.859685</td>\n",
       "      <td>Active</td>\n",
       "      <td>1</td>\n",
       "      <td>Enhanced Plan</td>\n",
       "      <td>365</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MEM00000003-01</td>\n",
       "      <td>1986</td>\n",
       "      <td>38</td>\n",
       "      <td>35-44</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>37</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>Active</td>\n",
       "      <td>1</td>\n",
       "      <td>Core Cover</td>\n",
       "      <td>365</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unique ID  Year of Birth  age age_group  gender_Female  gender_Male  \\\n",
       "0  MEM00000001-01           1968   56     55-64          False         True   \n",
       "1  MEM00000001-02           1967   57     55-64           True        False   \n",
       "2  MEM00000002-01           1976   48     45-54           True        False   \n",
       "3  MEM00000002-03           2005   19       <25          False         True   \n",
       "4  MEM00000003-01           1986   38     35-44          False         True   \n",
       "\n",
       "   gender_Other Original Date of Joining  membership_tenure_days  \\\n",
       "0         False               2018-09-05                    2096   \n",
       "1         False               2019-05-27                    1832   \n",
       "2         False               2020-09-11                    1359   \n",
       "3         False               2019-07-23                    1775   \n",
       "4         False               2024-04-25                      37   \n",
       "\n",
       "   membership_tenure_years Registration Status  is_active_member  \\\n",
       "0                 5.738535              Active                 1   \n",
       "1                 5.015743              Active                 1   \n",
       "2                 3.720739              Active                 1   \n",
       "3                 4.859685              Active                 1   \n",
       "4                 0.101300              Active                 1   \n",
       "\n",
       "  Scheme Category/ Section Name  contract_duration_days Lapse Date  has_lapsed  \n",
       "0                Advanced Cover                     365       None           0  \n",
       "1               Executive Cover                     365       None           0  \n",
       "2                 Premier Cover                     365       None           0  \n",
       "3                 Enhanced Plan                     365       None           0  \n",
       "4                    Core Cover                     365       None           0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_demographic_features(df, observation_date):\n",
    "    \"\"\"\n",
    "    Create demographic and membership-based features\n",
    "    \"\"\"\n",
    "    \n",
    "    observation_date = pd.to_datetime(observation_date)\n",
    "    features = df[['Unique ID']].drop_duplicates().copy()\n",
    "    \n",
    "    # Age calculation\n",
    "    features = features.merge(\n",
    "        df.groupby('Unique ID')['Year of Birth'].first(),\n",
    "        on='Unique ID'\n",
    "    )\n",
    "    features['age'] = observation_date.year - features['Year of Birth']\n",
    "    \n",
    "    # Age groups\n",
    "    features['age_group'] = pd.cut(\n",
    "        features['age'], \n",
    "        bins=[0, 25, 35, 45, 55, 65, 100],\n",
    "        labels=['<25', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "    )\n",
    "    \n",
    "   # Merge gender\n",
    "    features = features.merge(\n",
    "        df.groupby('Unique ID')['Gender'].first(),\n",
    "        on='Unique ID'\n",
    "    )\n",
    "\n",
    "    # One-hot encode all gender categories\n",
    "    gender_dummies = pd.get_dummies(features['Gender'], prefix='gender')\n",
    "\n",
    "    # Combine with main dataset\n",
    "    features = pd.concat([features, gender_dummies], axis=1)\n",
    "\n",
    "    # Optionally drop original gender column\n",
    "    features.drop(columns=['Gender'], inplace=True)\n",
    "    \n",
    "    # Membership tenure\n",
    "    features = features.merge(\n",
    "        df.groupby('Unique ID')['Original Date of Joining'].first(),\n",
    "        on='Unique ID'\n",
    "    )\n",
    "    features['Original Date of Joining'] = pd.to_datetime(features['Original Date of Joining'])\n",
    "    features['membership_tenure_days'] = (observation_date - features['Original Date of Joining']).dt.days\n",
    "    features['membership_tenure_years'] = features['membership_tenure_days'] / 365.25\n",
    "    \n",
    "    # Membership status\n",
    "    features = features.merge(\n",
    "        df.groupby('Unique ID')['Registration Status'].first(),\n",
    "        on='Unique ID'\n",
    "    )\n",
    "    features['is_active_member'] = (features['Registration Status'] == 'Active').astype(int)\n",
    "    \n",
    "    # Scheme information\n",
    "    features = features.merge(\n",
    "        df.groupby('Unique ID')['Scheme Category/ Section Name'].first(),\n",
    "        on='Unique ID'\n",
    "    )\n",
    "    \n",
    "    # Contract duration\n",
    "    df['Contract Start Date'] = pd.to_datetime(df['Contract Start Date'], errors='coerce')\n",
    "    df['Contract End Date'] = pd.to_datetime(df['Contract End Date'], errors='coerce')\n",
    "    \n",
    "    contract_info = df.groupby('Unique ID').agg({\n",
    "        'Contract Start Date': 'first',\n",
    "        'Contract End Date': 'last'\n",
    "    }).reset_index()\n",
    "    \n",
    "    contract_info['contract_duration_days'] = (\n",
    "        contract_info['Contract End Date'] - contract_info['Contract Start Date']\n",
    "    ).dt.days\n",
    "    \n",
    "    features = features.merge(contract_info[['Unique ID', 'contract_duration_days']], on='Unique ID', how='left')\n",
    "    \n",
    "    # Lapse indicator\n",
    "    features = features.merge(\n",
    "        df.groupby('Unique ID')['Lapse Date'].first(),\n",
    "        on='Unique ID'\n",
    "    )\n",
    "    features['has_lapsed'] = features['Lapse Date'].notna().astype(int)\n",
    "    \n",
    "    return features\n",
    "\n",
    "demographic_features = create_demographic_features(df, observation_date)\n",
    "print(f\"\\nFeatures created: {demographic_features.shape[1]}\")\n",
    "demographic_features.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3f83b7",
   "metadata": {},
   "source": [
    "## PHASE 2: MODEL TRAINING & EVALUATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a6ff3c",
   "metadata": {},
   "source": [
    "### STEP 5: Train-Test Split & Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ea98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def prepare_modeling_data(df, selected_features, target_col='is_at_risk'):\n",
    "    \"\"\"\n",
    "    Prepare final dataset for modeling\n",
    "    \"\"\"\n",
    "    # Select features and target\n",
    "    feature_cols = [col for col in selected_features if col in df.columns]\n",
    "    \n",
    "    # Handle any missing features\n",
    "    available_features = [col for col in feature_cols if col in df.columns]\n",
    "    print(f\"Using {len(available_features)} features for modeling\")\n",
    "    \n",
    "    X = df[available_features].copy()\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Handle non-numeric columns\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object':\n",
    "            # Convert to category codes\n",
    "            if X[col].nunique() < 50:\n",
    "                X[col] = X[col].astype('category').cat.codes\n",
    "            else:\n",
    "                X[col] = pd.Categorical(X[col]).codes\n",
    "        elif X[col].dtype == 'bool':\n",
    "            X[col] = X[col].astype(int)\n",
    "    \n",
    "    # Fill any remaining NaN\n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    # Train-test split (80-20)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    print(f\"\\nTarget distribution (Train):\")\n",
    "    print(y_train.value_counts(normalize=True))\n",
    "    print(f\"\\nTarget distribution (Test):\")\n",
    "    print(y_test.value_counts(normalize=True))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, available_features\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test, final_features = prepare_modeling_data(\n",
    "    processed_df, \n",
    "    selected_features, \n",
    "    target_col='is_at_risk'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed4e5c7",
   "metadata": {},
   "source": [
    "### STEP 6: Handle Class Imbalance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3742b496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "def balance_training_data(X_train, y_train, method='smote'):\n",
    "    \"\"\"\n",
    "    Balance the training data to handle class imbalance\n",
    "    \"\"\"\n",
    "    print(f\"Original class distribution:\")\n",
    "    print(y_train.value_counts())\n",
    "    print(f\"\\nAt-risk rate: {y_train.mean()*100:.2f}%\")\n",
    "    \n",
    "    if method == 'smote':\n",
    "        # SMOTE: Synthetic Minority Oversampling Technique\n",
    "        smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "        X_balanced, y_balanced = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "    elif method == 'combined':\n",
    "        # Combine SMOTE with undersampling\n",
    "        over = SMOTE(sampling_strategy=0.1, random_state=42)\n",
    "        under = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n",
    "        pipeline = ImbPipeline(steps=[('over', over), ('under', under)])\n",
    "        X_balanced, y_balanced = pipeline.fit_resample(X_train, y_train)\n",
    "    \n",
    "    else:\n",
    "        # No balancing\n",
    "        X_balanced, y_balanced = X_train.copy(), y_train.copy()\n",
    "    \n",
    "    print(f\"\\nBalanced class distribution:\")\n",
    "    print(pd.Series(y_balanced).value_counts())\n",
    "    print(f\"\\nBalanced at-risk rate: {y_balanced.mean()*100:.2f}%\")\n",
    "    \n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "# Balance training data\n",
    "X_train_balanced, y_train_balanced = balance_training_data(X_train, y_train, method='smote')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15526aaa",
   "metadata": {},
   "source": [
    "### STEP 7: Train Multiple Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_auc_score, average_precision_score,\n",
    "    precision_recall_curve, roc_curve\n",
    ")\n",
    "import time\n",
    "\n",
    "def train_baseline_models(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train multiple baseline models for comparison\n",
    "    \"\"\"\n",
    "    \n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'XGBoost': XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1]),\n",
    "            random_state=42,\n",
    "            eval_metric='logloss',\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'LightGBM': LGBMClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=7,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    trained_models = {}\n",
    "    \n",
    "    print(\"=== Training Models ===\\n\")\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Evaluate\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "        \n",
    "        # Classification report\n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        \n",
    "        results[name] = {\n",
    "            'roc_auc': roc_auc,\n",
    "            'pr_auc': pr_auc,\n",
    "            'precision': report['1']['precision'],\n",
    "            'recall': report['1']['recall'],\n",
    "            'f1_score': report['1']['f1-score'],\n",
    "            'training_time': training_time,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        trained_models[name] = model\n",
    "        \n",
    "        print(f\"  ROC-AUC: {roc_auc:.4f}\")\n",
    "        print(f\"  PR-AUC: {pr_auc:.4f}\")\n",
    "        print(f\"  Precision: {report['1']['precision']:.4f}\")\n",
    "        print(f\"  Recall: {report['1']['recall']:.4f}\")\n",
    "        print(f\"  F1-Score: {report['1']['f1-score']:.4f}\")\n",
    "        print(f\"  Training Time: {training_time:.2f}s\\n\")\n",
    "    \n",
    "    # Compare results\n",
    "    comparison_df = pd.DataFrame({\n",
    "        name: {\n",
    "            'ROC-AUC': results[name]['roc_auc'],\n",
    "            'PR-AUC': results[name]['pr_auc'],\n",
    "            'Precision': results[name]['precision'],\n",
    "            'Recall': results[name]['recall'],\n",
    "            'F1-Score': results[name]['f1_score'],\n",
    "            'Training Time (s)': results[name]['training_time']\n",
    "        }\n",
    "        for name in results\n",
    "    }).T.sort_values('PR-AUC', ascending=False)\n",
    "    \n",
    "    print(\"=== Model Comparison ===\")\n",
    "    print(comparison_df)\n",
    "    \n",
    "    return trained_models, results, comparison_df\n",
    "\n",
    "# Train all models\n",
    "trained_models, model_results, comparison_df = train_baseline_models(\n",
    "    X_train_balanced, y_train_balanced, X_test, y_test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36140d8b",
   "metadata": {},
   "source": [
    "### STEP 8: Hyperparameter Tuning (Best Model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9a10cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "def tune_best_model(X_train, y_train, model_type='lightgbm'):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning on the best performing model\n",
    "    \"\"\"\n",
    "    \n",
    "    if model_type == 'lightgbm':\n",
    "        param_distributions = {\n",
    "            'n_estimators': randint(100, 500),\n",
    "            'learning_rate': uniform(0.01, 0.2),\n",
    "            'max_depth': randint(3, 10),\n",
    "            'num_leaves': randint(20, 100),\n",
    "            'min_child_samples': randint(10, 50),\n",
    "            'subsample': uniform(0.6, 0.4),\n",
    "            'colsample_bytree': uniform(0.6, 0.4),\n",
    "            'reg_alpha': uniform(0, 1),\n",
    "            'reg_lambda': uniform(0, 1)\n",
    "        }\n",
    "        \n",
    "        base_model = LGBMClassifier(\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    elif model_type == 'xgboost':\n",
    "        param_distributions = {\n",
    "            'n_estimators': randint(100, 500),\n",
    "            'learning_rate': uniform(0.01, 0.2),\n",
    "            'max_depth': randint(3, 10),\n",
    "            'min_child_weight': randint(1, 10),\n",
    "            'subsample': uniform(0.6, 0.4),\n",
    "            'colsample_bytree': uniform(0.6, 0.4),\n",
    "            'gamma': uniform(0, 0.5),\n",
    "            'reg_alpha': uniform(0, 1),\n",
    "            'reg_lambda': uniform(0, 1)\n",
    "        }\n",
    "        \n",
    "        scale_pos_weight = len(y_train[y_train==0])/len(y_train[y_train==1])\n",
    "        base_model = XGBClassifier(\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_state=42,\n",
    "            eval_metric='logloss',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    print(f\"=== Hyperparameter Tuning for {model_type.upper()} ===\\n\")\n",
    "    \n",
    "    # Stratified K-Fold for cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Randomized search\n",
    "    random_search = RandomizedSearchCV(\n",
    "        base_model,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=50,  # number of parameter combinations to try\n",
    "        scoring='roc_auc',\n",
    "        cv=cv,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\nBest parameters:\")\n",
    "    for param, value in random_search.best_params_.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\nBest CV Score (ROC-AUC): {random_search.best_score_:.4f}\")\n",
    "    \n",
    "    return random_search.best_estimator_, random_search.best_params_\n",
    "\n",
    "# Identify best model from comparison\n",
    "best_model_name = comparison_df.index[0]\n",
    "print(f\"Best baseline model: {best_model_name}\\n\")\n",
    "\n",
    "# Tune the best model (using LightGBM as default if it's best, otherwise XGBoost)\n",
    "if 'LightGBM' in best_model_name or 'lightgbm' in best_model_name.lower():\n",
    "    best_model, best_params = tune_best_model(X_train_balanced, y_train_balanced, model_type='lightgbm')\n",
    "elif 'XGBoost' in best_model_name or 'xgboost' in best_model_name.lower():\n",
    "    best_model, best_params = tune_best_model(X_train_balanced, y_train_balanced, model_type='xgboost')\n",
    "else:\n",
    "    # Use LightGBM as default for tuning\n",
    "    print(\"Using LightGBM for hyperparameter tuning...\")\n",
    "    best_model, best_params = tune_best_model(X_train_balanced, y_train_balanced, model_type='lightgbm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffb51b3",
   "metadata": {},
   "source": [
    "### STEP 9: Final Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d7c308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_final_model(model, X_test, y_test, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of the final model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    print(\"=== FINAL MODEL EVALUATION ===\\n\")\n",
    "    \n",
    "    # === 1. CLASSIFICATION METRICS ===\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Not At-Risk', 'At-Risk']))\n",
    "    \n",
    "    # === 2. CONFUSION MATRIX ===\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"\\nTrue Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    \n",
    "    # === 3. ROC-AUC ===\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\n",
    "    \n",
    "    # === 4. PRECISION-RECALL AUC ===\n",
    "    pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "    print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "    \n",
    "    # === 5. BUSINESS METRICS ===\n",
    "    # Capture rate in top deciles\n",
    "    df_scores = pd.DataFrame({\n",
    "        'actual': y_test,\n",
    "        'predicted_proba': y_pred_proba\n",
    "    }).sort_values('predicted_proba', ascending=False)\n",
    "    \n",
    "    total_positives = df_scores['actual'].sum()\n",
    "    \n",
    "    print(\"\\n=== Capture Rate Analysis ===\")\n",
    "    for decile in [10, 20, 30]:\n",
    "        n = int(len(df_scores) * (decile/100))\n",
    "        top_decile = df_scores.head(n)\n",
    "        captured = top_decile['actual'].sum()\n",
    "        capture_rate = (captured / total_positives * 100) if total_positives > 0 else 0\n",
    "        print(f\"Top {decile}%: Captures {capture_rate:.1f}% of at-risk members ({captured}/{total_positives})\")\n",
    "    \n",
    "    # === 6. VISUALIZATIONS ===\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Confusion Matrix Heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Confusion Matrix')\n",
    "    axes[0, 0].set_ylabel('Actual')\n",
    "    axes[0, 0].set_xlabel('Predicted')\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    axes[0, 1].plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "    axes[0, 1].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    axes[0, 1].set_xlabel('False Positive Rate')\n",
    "    axes[0, 1].set_ylabel('True Positive Rate')\n",
    "    axes[0, 1].set_title('ROC Curve')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    axes[1, 0].plot(recall, precision, label=f'PR Curve (AUC = {pr_auc:.4f})')\n",
    "    axes[1, 0].set_xlabel('Recall')\n",
    "    axes[1, 0].set_ylabel('Precision')\n",
    "    axes[1, 0].set_title('Precision-Recall Curve')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Prediction Distribution\n",
    "    axes[1, 1].hist(y_pred_proba[y_test == 0], bins=50, alpha=0.5, label='Not At-Risk', density=True)\n",
    "    axes[1, 1].hist(y_pred_proba[y_test == 1], bins=50, alpha=0.5, label='At-Risk', density=True)\n",
    "    axes[1, 1].axvline(threshold, color='r', linestyle='--', label=f'Threshold = {threshold}')\n",
    "    axes[1, 1].set_xlabel('Predicted Probability')\n",
    "    axes[1, 1].set_ylabel('Density')\n",
    "    axes[1, 1].set_title('Prediction Probability Distribution')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# Evaluate final model\n",
    "final_evaluation = evaluate_final_model(best_model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea46b9c4",
   "metadata": {},
   "source": [
    "## PHASE 3: CAUSAL ANALYTICS & EXPLORATORY ANALYSIS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ead1b",
   "metadata": {},
   "source": [
    "### STEP 10: Feature Importance Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276861b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, feature_names, top_n=30):\n",
    "    \"\"\"\n",
    "    Analyze and visualize feature importance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get feature importance\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        # For linear models, use absolute coefficients\n",
    "        importances = np.abs(model.coef_[0])\n",
    "    else:\n",
    "        print(\"Model does not support feature importance extraction\")\n",
    "        return None\n",
    "    \n",
    "    # Create importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"=== Top Feature Importances ===\\n\")\n",
    "    print(importance_df.head(top_n))\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = importance_df.head(top_n)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Top {top_n} Most Important Features')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Analyze feature importance\n",
    "feature_importance_df = analyze_feature_importance(best_model, final_features, top_n=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeebbacb",
   "metadata": {},
   "source": [
    "### STEP 11: Causal Analytics - Root Cause Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0bfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_analytics(df, target_col='is_at_risk'):\n",
    "    \"\"\"\n",
    "    Perform causal analytics to identify root causes of high risk\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== CAUSAL ANALYTICS: ROOT CAUSE ANALYSIS ===\\n\")\n",
    "    \n",
    "    # === 1. DEMOGRAPHIC FACTORS ===\n",
    "    print(\"1. DEMOGRAPHIC FACTORS\\n\")\n",
    "    \n",
    "    # Age groups\n",
    "    if 'age_group' in df.columns:\n",
    "        age_risk = df.groupby('age_group')[target_col].agg(['mean', 'count'])\n",
    "        age_risk.columns = ['At-Risk Rate', 'Count']\n",
    "        age_risk = age_risk.sort_values('At-Risk Rate', ascending=False)\n",
    "        print(\"At-Risk Rate by Age Group:\")\n",
    "        print(age_risk)\n",
    "        print()\n",
    "    \n",
    "    # Gender\n",
    "    gender_cols = [col for col in df.columns if 'gender_' in col.lower()]\n",
    "    if gender_cols:\n",
    "        for col in gender_cols:\n",
    "            gender_name = col.replace('gender_', '').replace('_', ' ').title()\n",
    "            risk_rate = df[df[col] == True][target_col].mean() if df[col].dtype == bool else df[df[col] == 1][target_col].mean()\n",
    "            count = (df[col] == True).sum() if df[col].dtype == bool else (df[col] == 1).sum()\n",
    "            print(f\"{gender_name}: {risk_rate*100:.2f}% at-risk (n={count})\")\n",
    "        print()\n",
    "    \n",
    "    # === 2. CLAIMS HISTORY FACTORS ===\n",
    "    print(\"2. CLAIMS HISTORY FACTORS\\n\")\n",
    "    \n",
    "    # Recent activity\n",
    "    if 'claims_count_3m' in df.columns:\n",
    "        df['has_recent_claims'] = (df['claims_count_3m'] > 0).astype(int)\n",
    "        recent_risk = df.groupby('has_recent_claims')[target_col].agg(['mean', 'count'])\n",
    "        recent_risk.index = ['No Recent Claims', 'Has Recent Claims']\n",
    "        recent_risk.columns = ['At-Risk Rate', 'Count']\n",
    "        print(\"At-Risk Rate by Recent Claims (3 months):\")\n",
    "        print(recent_risk)\n",
    "        print()\n",
    "    \n",
    "    # High cost claims\n",
    "    if 'high_cost_claims_count' in df.columns:\n",
    "        df['has_high_cost'] = (df['high_cost_claims_count'] > 0).astype(int)\n",
    "        high_cost_risk = df.groupby('has_high_cost')[target_col].agg(['mean', 'count'])\n",
    "        high_cost_risk.index = ['No High-Cost Claims', 'Has High-Cost Claims']\n",
    "        high_cost_risk.columns = ['At-Risk Rate', 'Count']\n",
    "        print(\"At-Risk Rate by High-Cost Claims:\")\n",
    "        print(high_cost_risk)\n",
    "        print()\n",
    "    \n",
    "    # === 3. CONDITION FACTORS ===\n",
    "    print(\"3. CONDITION FACTORS\\n\")\n",
    "    \n",
    "    if 'has_high_risk_condition_history' in df.columns:\n",
    "        condition_risk = df.groupby('has_high_risk_condition_history')[target_col].agg(['mean', 'count'])\n",
    "        condition_risk.index = ['No High-Risk Conditions', 'Has High-Risk Conditions']\n",
    "        condition_risk.columns = ['At-Risk Rate', 'Count']\n",
    "        print(\"At-Risk Rate by High-Risk Condition History:\")\n",
    "        print(condition_risk)\n",
    "        print()\n",
    "    \n",
    "    # === 4. MEMBERSHIP FACTORS ===\n",
    "    print(\"4. MEMBERSHIP FACTORS\\n\")\n",
    "    \n",
    "    if 'membership_tenure_years' in df.columns:\n",
    "        df['tenure_group'] = pd.cut(\n",
    "            df['membership_tenure_years'],\n",
    "            bins=[0, 1, 3, 5, 10, 100],\n",
    "            labels=['<1 year', '1-3 years', '3-5 years', '5-10 years', '10+ years']\n",
    "        )\n",
    "        tenure_risk = df.groupby('tenure_group')[target_col].agg(['mean', 'count'])\n",
    "        tenure_risk.columns = ['At-Risk Rate', 'Count']\n",
    "        print(\"At-Risk Rate by Membership Tenure:\")\n",
    "        print(tenure_risk)\n",
    "        print()\n",
    "    \n",
    "    # === 5. COMPOSITE RISK SCORE ANALYSIS ===\n",
    "    print(\"5. COMPOSITE RISK SCORE ANALYSIS\\n\")\n",
    "    \n",
    "    if 'composite_risk_score' in df.columns:\n",
    "        df['risk_quartile'] = pd.qcut(\n",
    "            df['composite_risk_score'],\n",
    "            q=4,\n",
    "            labels=['Q1 (Lowest)', 'Q2', 'Q3', 'Q4 (Highest)']\n",
    "        )\n",
    "        quartile_risk = df.groupby('risk_quartile')[target_col].agg(['mean', 'count'])\n",
    "        quartile_risk.columns = ['At-Risk Rate', 'Count']\n",
    "        print(\"At-Risk Rate by Risk Score Quartile:\")\n",
    "        print(quartile_risk)\n",
    "        print()\n",
    "    \n",
    "    # === 6. CORRELATION ANALYSIS ===\n",
    "    print(\"6. TOP CORRELATIONS WITH AT-RISK STATUS\\n\")\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    correlations = df[numeric_cols].corrwith(df[target_col]).abs().sort_values(ascending=False)\n",
    "    top_correlations = correlations.head(15)\n",
    "    print(top_correlations)\n",
    "    \n",
    "    return {\n",
    "        'age_risk': age_risk if 'age_group' in df.columns else None,\n",
    "        'recent_claims_risk': recent_risk if 'claims_count_3m' in df.columns else None,\n",
    "        'high_cost_risk': high_cost_risk if 'high_cost_claims_count' in df.columns else None,\n",
    "        'condition_risk': condition_risk if 'has_high_risk_condition_history' in df.columns else None,\n",
    "        'tenure_risk': tenure_risk if 'membership_tenure_years' in df.columns else None,\n",
    "        'correlations': top_correlations\n",
    "    }\n",
    "\n",
    "# Perform causal analytics\n",
    "causal_results = causal_analytics(processed_df, target_col='is_at_risk')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f9f14",
   "metadata": {},
   "source": [
    "### STEP 12: Cohort Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed0e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohort_analysis(df, target_col='is_at_risk'):\n",
    "    \"\"\"\n",
    "    Identify high-risk cohorts for targeted interventions\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== COHORT ANALYSIS: HIGH-RISK GROUPS ===\\n\")\n",
    "    \n",
    "    # Create cohort definitions\n",
    "    cohorts = []\n",
    "    \n",
    "    # Cohort 1: Recent High Activity + High Cost\n",
    "    if all(col in df.columns for col in ['claims_count_3m', 'high_cost_claims_count']):\n",
    "        cohort1 = df[\n",
    "            (df['claims_count_3m'] >= 2) & \n",
    "            (df['high_cost_claims_count'] > 0)\n",
    "        ].copy()\n",
    "        if len(cohort1) > 0:\n",
    "            cohorts.append({\n",
    "                'name': 'Recent High Activity + High Cost',\n",
    "                'criteria': 'â‰¥2 claims in 3m AND has high-cost claims',\n",
    "                'size': len(cohort1),\n",
    "                'at_risk_rate': cohort1[target_col].mean(),\n",
    "                'avg_risk_score': cohort1['composite_risk_score'].mean() if 'composite_risk_score' in cohort1.columns else None,\n",
    "                'members': cohort1\n",
    "            })\n",
    "    \n",
    "    # Cohort 2: High-Risk Condition History\n",
    "    if 'has_high_risk_condition_history' in df.columns:\n",
    "        cohort2 = df[df['has_high_risk_condition_history'] == 1].copy()\n",
    "        if len(cohort2) > 0:\n",
    "            cohorts.append({\n",
    "                'name': 'High-Risk Condition History',\n",
    "                'criteria': 'Has high-risk condition in history',\n",
    "                'size': len(cohort2),\n",
    "                'at_risk_rate': cohort2[target_col].mean(),\n",
    "                'avg_risk_score': cohort2['composite_risk_score'].mean() if 'composite_risk_score' in cohort2.columns else None,\n",
    "                'members': cohort2\n",
    "            })\n",
    "    \n",
    "    # Cohort 3: Accelerating Claims\n",
    "    if all(col in df.columns for col in ['claims_count_6m', 'claims_count_24m']):\n",
    "        df['claim_acceleration'] = (df['claims_count_6m'] / 6) / ((df['claims_count_24m'] - df['claims_count_6m']) / 18 + 0.1)\n",
    "        cohort3 = df[df['claim_acceleration'] > 2].copy()\n",
    "        if len(cohort3) > 0:\n",
    "            cohorts.append({\n",
    "                'name': 'Accelerating Claims',\n",
    "                'criteria': 'Claim frequency accelerating (recent > 2x historical)',\n",
    "                'size': len(cohort3),\n",
    "                'at_risk_rate': cohort3[target_col].mean(),\n",
    "                'avg_risk_score': cohort3['composite_risk_score'].mean() if 'composite_risk_score' in cohort3.columns else None,\n",
    "                'members': cohort3\n",
    "            })\n",
    "    \n",
    "    # Cohort 4: High Composite Risk Score\n",
    "    if 'composite_risk_score' in df.columns:\n",
    "        risk_threshold = df['composite_risk_score'].quantile(0.90)\n",
    "        cohort4 = df[df['composite_risk_score'] >= risk_threshold].copy()\n",
    "        if len(cohort4) > 0:\n",
    "            cohorts.append({\n",
    "                'name': 'Top 10% Risk Score',\n",
    "                'criteria': f'Composite risk score â‰¥ {risk_threshold:.2f} (90th percentile)',\n",
    "                'size': len(cohort4),\n",
    "                'at_risk_rate': cohort4[target_col].mean(),\n",
    "                'avg_risk_score': cohort4['composite_risk_score'].mean(),\n",
    "                'members': cohort4\n",
    "            })\n",
    "    \n",
    "    # Cohort 5: Age + Condition Combination\n",
    "    if all(col in df.columns for col in ['age', 'has_high_risk_condition_history']):\n",
    "        cohort5 = df[\n",
    "            (df['age'] >= 55) & \n",
    "            (df['has_high_risk_condition_history'] == 1)\n",
    "        ].copy()\n",
    "        if len(cohort5) > 0:\n",
    "            cohorts.append({\n",
    "                'name': 'Older Adults with High-Risk Conditions',\n",
    "                'criteria': 'Age â‰¥55 AND has high-risk condition history',\n",
    "                'size': len(cohort5),\n",
    "                'at_risk_rate': cohort5[target_col].mean(),\n",
    "                'avg_risk_score': cohort5['composite_risk_score'].mean() if 'composite_risk_score' in cohort5.columns else None,\n",
    "                'members': cohort5\n",
    "            })\n",
    "    \n",
    "    # Display cohort summary\n",
    "    cohort_summary = pd.DataFrame({\n",
    "        'Cohort': [c['name'] for c in cohorts],\n",
    "        'Size': [c['size'] for c in cohorts],\n",
    "        'At-Risk Rate': [f\"{c['at_risk_rate']*100:.2f}%\" for c in cohorts],\n",
    "        'Avg Risk Score': [f\"{c['avg_risk_score']:.2f}\" if c['avg_risk_score'] else \"N/A\" for c in cohorts]\n",
    "    })\n",
    "    \n",
    "    print(\"Cohort Summary:\")\n",
    "    print(cohort_summary)\n",
    "    print()\n",
    "    \n",
    "    # Visualize cohort comparison\n",
    "    if len(cohorts) > 0:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # At-risk rate by cohort\n",
    "        cohort_names = [c['name'] for c in cohorts]\n",
    "        at_risk_rates = [c['at_risk_rate']*100 for c in cohorts]\n",
    "        \n",
    "        axes[0].barh(cohort_names, at_risk_rates)\n",
    "        axes[0].set_xlabel('At-Risk Rate (%)')\n",
    "        axes[0].set_title('At-Risk Rate by Cohort')\n",
    "        axes[0].grid(True, axis='x')\n",
    "        \n",
    "        # Cohort sizes\n",
    "        cohort_sizes = [c['size'] for c in cohorts]\n",
    "        axes[1].barh(cohort_names, cohort_sizes)\n",
    "        axes[1].set_xlabel('Number of Members')\n",
    "        axes[1].set_title('Cohort Sizes')\n",
    "        axes[1].grid(True, axis='x')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return cohorts\n",
    "\n",
    "# Perform cohort analysis\n",
    "high_risk_cohorts = cohort_analysis(processed_df, target_col='is_at_risk')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050b77ed",
   "metadata": {},
   "source": [
    "### STEP 13: Intervention Recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51326451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_intervention_recommendations(cohorts, feature_importance_df, causal_results):\n",
    "    \"\"\"\n",
    "    Generate actionable intervention recommendations based on analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== INTERVENTION RECOMMENDATIONS ===\\n\")\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # === 1. BASED ON TOP FEATURES ===\n",
    "    print(\"1. INTERVENTIONS BASED ON KEY RISK FACTORS\\n\")\n",
    "    \n",
    "    if feature_importance_df is not None:\n",
    "        top_features = feature_importance_df.head(10)\n",
    "        \n",
    "        for idx, row in top_features.iterrows():\n",
    "            feature = row['feature']\n",
    "            importance = row['importance']\n",
    "            \n",
    "            # Map features to interventions\n",
    "            if 'recent_activity' in feature.lower() or 'claims_count_3m' in feature.lower():\n",
    "                recommendations.append({\n",
    "                    'priority': 'HIGH',\n",
    "                    'target_group': 'Members with recent high claim activity',\n",
    "                    'intervention': 'Proactive health check-ups and wellness consultations',\n",
    "                    'rationale': f'Recent activity is a strong predictor (importance: {importance:.4f})',\n",
    "                    'expected_impact': 'Early detection and prevention of complications'\n",
    "                })\n",
    "            \n",
    "            elif 'high_risk_condition' in feature.lower() or 'chronic' in feature.lower():\n",
    "                recommendations.append({\n",
    "                    'priority': 'HIGH',\n",
    "                    'target_group': 'Members with high-risk condition history',\n",
    "                    'intervention': 'Disease management programs and regular monitoring',\n",
    "                    'rationale': f'Condition history is critical (importance: {importance:.4f})',\n",
    "                    'expected_impact': 'Prevent condition progression and complications'\n",
    "                })\n",
    "            \n",
    "            elif 'composite_risk_score' in feature.lower():\n",
    "                recommendations.append({\n",
    "                    'priority': 'MEDIUM',\n",
    "                    'target_group': 'Members in top risk quartiles',\n",
    "                    'intervention': 'Comprehensive risk assessment and personalized care plans',\n",
    "                    'rationale': f'Composite score captures multiple risk factors (importance: {importance:.4f})',\n",
    "                    'expected_impact': 'Holistic risk management'\n",
    "                })\n",
    "            \n",
    "            elif 'membership_tenure' in feature.lower():\n",
    "                recommendations.append({\n",
    "                    'priority': 'MEDIUM',\n",
    "                    'target_group': 'New members (<1 year)',\n",
    "                    'intervention': 'Enhanced onboarding and health screening',\n",
    "                    'rationale': f'Membership tenure affects risk patterns (importance: {importance:.4f})',\n",
    "                    'expected_impact': 'Early identification and engagement'\n",
    "                })\n",
    "    \n",
    "    # === 2. BASED ON COHORTS ===\n",
    "    print(\"\\n2. COHORT-SPECIFIC INTERVENTIONS\\n\")\n",
    "    \n",
    "    for cohort in cohorts:\n",
    "        cohort_name = cohort['name']\n",
    "        at_risk_rate = cohort['at_risk_rate']\n",
    "        \n",
    "        if 'Recent High Activity' in cohort_name:\n",
    "            recommendations.append({\n",
    "                'priority': 'HIGH',\n",
    "                'target_group': cohort_name,\n",
    "                'intervention': 'Immediate case management and care coordination',\n",
    "                'rationale': f'{at_risk_rate*100:.1f}% at-risk rate in this cohort',\n",
    "                'expected_impact': 'Prevent escalation of claims and conditions'\n",
    "            })\n",
    "        \n",
    "        elif 'High-Risk Condition' in cohort_name:\n",
    "            recommendations.append({\n",
    "                'priority': 'HIGH',\n",
    "                'target_group': cohort_name,\n",
    "                'intervention': 'Specialized disease management and specialist referrals',\n",
    "                'rationale': f'{at_risk_rate*100:.1f}% at-risk rate - condition-specific care needed',\n",
    "                'expected_impact': 'Better disease control and reduced complications'\n",
    "            })\n",
    "        \n",
    "        elif 'Accelerating' in cohort_name:\n",
    "            recommendations.append({\n",
    "                'priority': 'HIGH',\n",
    "                'target_group': cohort_name,\n",
    "                'intervention': 'Urgent health assessment and intervention planning',\n",
    "                'rationale': f'{at_risk_rate*100:.1f}% at-risk rate - claims accelerating',\n",
    "                'expected_impact': 'Arrest the acceleration trend'\n",
    "            })\n",
    "        \n",
    "        elif 'Older Adults' in cohort_name:\n",
    "            recommendations.append({\n",
    "                'priority': 'MEDIUM',\n",
    "                'target_group': cohort_name,\n",
    "                'intervention': 'Geriatric care management and preventive screenings',\n",
    "                'rationale': f'{at_risk_rate*100:.1f}% at-risk rate - age-related risk',\n",
    "                'expected_impact': 'Age-appropriate care and prevention'\n",
    "            })\n",
    "    \n",
    "    # === 3. GENERAL RECOMMENDATIONS ===\n",
    "    print(\"\\n3. GENERAL RECOMMENDATIONS\\n\")\n",
    "    \n",
    "    general_recommendations = [\n",
    "        {\n",
    "            'priority': 'MEDIUM',\n",
    "            'target_group': 'All members with risk score > 50th percentile',\n",
    "            'intervention': 'Regular health assessments and wellness programs',\n",
    "            'rationale': 'Preventive care for moderate-risk members',\n",
    "            'expected_impact': 'Early intervention and risk reduction'\n",
    "        },\n",
    "        {\n",
    "            'priority': 'LOW',\n",
    "            'target_group': 'Low-risk members',\n",
    "            'intervention': 'Maintain wellness programs and preventive care',\n",
    "            'rationale': 'Keep low-risk members healthy',\n",
    "            'expected_impact': 'Prevent risk escalation'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    recommendations.extend(general_recommendations)\n",
    "    \n",
    "    # === 4. BENEFIT DESIGN RECOMMENDATIONS ===\n",
    "    print(\"\\n4. BENEFIT DESIGN RECOMMENDATIONS\\n\")\n",
    "    \n",
    "    benefit_recommendations = [\n",
    "        {\n",
    "            'recommendation': 'Enhanced preventive care coverage',\n",
    "            'rationale': 'Early detection reduces high-cost claims',\n",
    "            'target': 'All members, especially high-risk cohorts'\n",
    "        },\n",
    "        {\n",
    "            'recommendation': 'Disease management program incentives',\n",
    "            'rationale': 'Encourage participation in chronic disease management',\n",
    "            'target': 'Members with high-risk conditions'\n",
    "        },\n",
    "        {\n",
    "            'recommendation': 'Wellness program rewards',\n",
    "            'rationale': 'Incentivize healthy behaviors',\n",
    "            'target': 'Moderate-risk members'\n",
    "        },\n",
    "        {\n",
    "            'recommendation': 'Care coordination services',\n",
    "            'rationale': 'Reduce fragmented care and improve outcomes',\n",
    "            'target': 'High-risk members with multiple conditions'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Display recommendations\n",
    "    rec_df = pd.DataFrame(recommendations)\n",
    "    \n",
    "    print(\"Intervention Recommendations:\")\n",
    "    print(rec_df.to_string(index=False))\n",
    "    print()\n",
    "    \n",
    "    print(\"Benefit Design Recommendations:\")\n",
    "    for rec in benefit_recommendations:\n",
    "        print(f\"\\nâ€¢ {rec['recommendation']}\")\n",
    "        print(f\"  Rationale: {rec['rationale']}\")\n",
    "        print(f\"  Target: {rec['target']}\")\n",
    "    \n",
    "    # Priority summary\n",
    "    print(\"\\n=== PRIORITY SUMMARY ===\\n\")\n",
    "    priority_summary = rec_df.groupby('priority').size()\n",
    "    print(priority_summary)\n",
    "    \n",
    "    return recommendations, benefit_recommendations\n",
    "\n",
    "# Generate recommendations\n",
    "interventions, benefit_design = generate_intervention_recommendations(\n",
    "    high_risk_cohorts, \n",
    "    feature_importance_df, \n",
    "    causal_results\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802fdd7b",
   "metadata": {},
   "source": [
    "### STEP 14: Model Deployment Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c61f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model_and_artifacts(model, feature_names, model_results, best_params):\n",
    "    \"\"\"\n",
    "    Save the trained model and all necessary artifacts for deployment\n",
    "    \"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save model\n",
    "    model_filename = f'at_risk_model_{timestamp}.pkl'\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(f\"âœ… Model saved: {model_filename}\")\n",
    "    \n",
    "    # Save feature names\n",
    "    features_filename = f'model_features_{timestamp}.json'\n",
    "    with open(features_filename, 'w') as f:\n",
    "        json.dump(feature_names, f)\n",
    "    print(f\"âœ… Features saved: {features_filename}\")\n",
    "    \n",
    "    # Save model metadata\n",
    "    metadata = {\n",
    "        'model_type': type(model).__name__,\n",
    "        'training_date': timestamp,\n",
    "        'features': feature_names,\n",
    "        'n_features': len(feature_names),\n",
    "        'best_parameters': best_params,\n",
    "        'performance': {\n",
    "            'roc_auc': float(model_results.get('roc_auc', 0)),\n",
    "            'pr_auc': float(model_results.get('pr_auc', 0)),\n",
    "            'precision': float(model_results.get('precision', 0)),\n",
    "            'recall': float(model_results.get('recall', 0)),\n",
    "            'f1_score': float(model_results.get('f1_score', 0))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_filename = f'model_metadata_{timestamp}.json'\n",
    "    with open(metadata_filename, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"âœ… Metadata saved: {metadata_filename}\")\n",
    "    \n",
    "    return model_filename, features_filename, metadata_filename\n",
    "\n",
    "# Save model artifacts\n",
    "model_file, features_file, metadata_file = save_model_and_artifacts(\n",
    "    best_model,\n",
    "    final_features,\n",
    "    model_results.get(best_model_name, {}),\n",
    "    best_params\n",
    ")\n",
    "\n",
    "print(\"\\n=== MODEL DEPLOYMENT READY ===\")\n",
    "print(f\"Model: {model_file}\")\n",
    "print(f\"Features: {features_file}\")\n",
    "print(f\"Metadata: {metadata_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d32638f",
   "metadata": {},
   "source": [
    "### STEP 15: Prediction Function for New Members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc8646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_at_risk(member_data, model, feature_names, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict at-risk status for a new member or set of members\n",
    "    \n",
    "    Parameters:\n",
    "    - member_data: DataFrame with member features\n",
    "    - model: Trained model\n",
    "    - feature_names: List of feature names used in training\n",
    "    - threshold: Probability threshold for classification\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with predictions and probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select and align features\n",
    "    X = member_data[feature_names].copy()\n",
    "    \n",
    "    # Handle missing features (fill with 0)\n",
    "    for feature in feature_names:\n",
    "        if feature not in X.columns:\n",
    "            X[feature] = 0\n",
    "    \n",
    "    # Ensure correct order\n",
    "    X = X[feature_names]\n",
    "    \n",
    "    # Handle non-numeric columns\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object':\n",
    "            if X[col].nunique() < 50:\n",
    "                X[col] = X[col].astype('category').cat.codes\n",
    "            else:\n",
    "                X[col] = pd.Categorical(X[col]).codes\n",
    "        elif X[col].dtype == 'bool':\n",
    "            X[col] = X[col].astype(int)\n",
    "    \n",
    "    # Fill NaN\n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    # Predict\n",
    "    probabilities = model.predict_proba(X)[:, 1]\n",
    "    predictions = (probabilities >= threshold).astype(int)\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results = pd.DataFrame({\n",
    "        'member_id': member_data.index if 'Unique ID' not in member_data.columns else member_data['Unique ID'],\n",
    "        'at_risk_probability': probabilities,\n",
    "        'is_at_risk': predictions,\n",
    "        'risk_level': pd.cut(\n",
    "            probabilities,\n",
    "            bins=[0, 0.3, 0.6, 1.0],\n",
    "            labels=['Low', 'Medium', 'High']\n",
    "        )\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Predict on test set\n",
    "print(\"=== EXAMPLE PREDICTIONS ===\")\n",
    "test_predictions = predict_at_risk(\n",
    "    processed_df.loc[X_test.index].copy(),\n",
    "    best_model,\n",
    "    final_features,\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "print(f\"\\nPrediction Summary:\")\n",
    "print(test_predictions['risk_level'].value_counts())\n",
    "print(f\"\\nSample Predictions:\")\n",
    "print(test_predictions.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e33f25",
   "metadata": {},
   "source": [
    "## SUMMARY & NEXT STEPS\n",
    "\n",
    "### Project Completion Summary\n",
    "\n",
    "âœ… **Completed:**\n",
    "1. Data preparation and feature engineering (58 features)\n",
    "2. Model training with 5 algorithms\n",
    "3. Hyperparameter tuning for best model\n",
    "4. Comprehensive model evaluation\n",
    "5. Feature importance analysis\n",
    "6. Causal analytics and root cause identification\n",
    "7. Cohort analysis for high-risk groups\n",
    "8. Intervention recommendations\n",
    "9. Model deployment preparation\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Best Model**: [Check comparison_df for best performing model]\n",
    "2. **Top Risk Factors**: [Check feature_importance_df]\n",
    "3. **High-Risk Cohorts**: [Check high_risk_cohorts]\n",
    "4. **Intervention Priorities**: [Check interventions]\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Model Monitoring**: Set up monitoring for model performance over time\n",
    "2. **A/B Testing**: Test interventions on identified high-risk cohorts\n",
    "3. **Feedback Loop**: Collect outcomes data to improve model\n",
    "4. **Production Deployment**: Deploy model to production environment\n",
    "5. **Stakeholder Reporting**: Create dashboards for business users\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c7d551",
   "metadata": {},
   "source": [
    "#### 3.2 Historical Claims Features (CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f82f244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_15380\\29807541.py:102: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  trends = claims_df.groupby('Claimant Unique ID').apply(calculate_trend).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claims features created: 26\n"
     ]
    }
   ],
   "source": [
    "def create_claims_history_features(claims_df, observation_date):\n",
    "    \"\"\"\n",
    "    Create comprehensive historical claims features\n",
    "    \"\"\"\n",
    "    \n",
    "    observation_date = pd.to_datetime(observation_date)\n",
    "    claims_df['Paid Date'] = pd.to_datetime(claims_df['Paid Date'])\n",
    "    \n",
    "    # Calculate time periods\n",
    "    claims_df['months_before_observation'] = (\n",
    "        (observation_date.year - claims_df['Paid Date'].dt.year) * 12 +\n",
    "        (observation_date.month - claims_df['Paid Date'].dt.month)\n",
    "    )\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    # === 1. BASIC CLAIM COUNTS & AMOUNTS ===\n",
    "    basic_agg = claims_df.groupby('Claimant Unique ID').agg({\n",
    "        'Claim ID': 'count',  # total claims\n",
    "        'Claim Amount': ['sum', 'mean', 'median', 'std', 'min', 'max'],\n",
    "    }).reset_index()\n",
    "    \n",
    "    basic_agg.columns = ['Unique ID', 'total_claims_count', 'total_claim_amount', \n",
    "                         'avg_claim_amount', 'median_claim_amount', 'std_claim_amount',\n",
    "                         'min_claim_amount', 'max_claim_amount']\n",
    "    \n",
    "    features_list.append(basic_agg)\n",
    "    \n",
    "    # === 2. TIME-WINDOWED FEATURES ===\n",
    "    time_windows = [3, 6, 12, 24]  # months\n",
    "    \n",
    "    for window in time_windows:\n",
    "        window_claims = claims_df[claims_df['months_before_observation'] <= window]\n",
    "        \n",
    "        window_agg = window_claims.groupby('Claimant Unique ID').agg({\n",
    "            'Claim ID': 'count',\n",
    "            'Claim Amount': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        window_agg.columns = ['Unique ID', f'claims_count_{window}m', f'total_amount_{window}m']\n",
    "        \n",
    "        features_list.append(window_agg)\n",
    "    \n",
    "    # === 3. TEMPORAL PATTERNS ===\n",
    "    temporal = claims_df.groupby('Claimant Unique ID').agg({\n",
    "        'Paid Date': ['min', 'max', 'count']\n",
    "    }).reset_index()\n",
    "    \n",
    "    temporal.columns = ['Unique ID', 'first_claim_date', 'last_claim_date', 'claim_count']\n",
    "    temporal['days_since_first_claim'] = (observation_date - temporal['first_claim_date']).dt.days\n",
    "    temporal['days_since_last_claim'] = (observation_date - temporal['last_claim_date']).dt.days\n",
    "    temporal['claim_duration_days'] = (temporal['last_claim_date'] - temporal['first_claim_date']).dt.days\n",
    "    temporal['avg_days_between_claims'] = temporal['claim_duration_days'] / (temporal['claim_count'] - 1)\n",
    "    temporal['avg_days_between_claims'] = temporal['avg_days_between_claims'].fillna(0)\n",
    "    \n",
    "    features_list.append(temporal[['Unique ID', 'days_since_first_claim', 'days_since_last_claim', \n",
    "                                   'claim_duration_days', 'avg_days_between_claims']])\n",
    "    \n",
    "    # === 4. CLAIM FREQUENCY TRENDS ===\n",
    "    # Claims per month over time\n",
    "    claims_df['claim_year_month'] = claims_df['Paid Date'].dt.to_period('M')\n",
    "    monthly_claims = claims_df.groupby(['Claimant Unique ID', 'claim_year_month']).size().reset_index(name='monthly_count')\n",
    "    \n",
    "    freq_features = monthly_claims.groupby('Claimant Unique ID').agg({\n",
    "        'monthly_count': ['mean', 'std', 'max']\n",
    "    }).reset_index()\n",
    "    \n",
    "    freq_features.columns = ['Unique ID', 'avg_monthly_claim_frequency', \n",
    "                            'std_monthly_claim_frequency', 'max_monthly_claims']\n",
    "    \n",
    "    features_list.append(freq_features)\n",
    "    \n",
    "    # === 5. CLAIM SEVERITY INDICATORS ===\n",
    "    # High-cost claims\n",
    "    high_cost_threshold = claims_df['Claim Amount'].quantile(0.75)\n",
    "    claims_df['is_high_cost_claim'] = (claims_df['Claim Amount'] > high_cost_threshold).astype(int)\n",
    "    \n",
    "    severity = claims_df.groupby('Claimant Unique ID').agg({\n",
    "        'is_high_cost_claim': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    severity.columns = ['Unique ID', 'high_cost_claims_count']\n",
    "    \n",
    "    # Cost volatility\n",
    "    cost_volatility = claims_df.groupby('Claimant Unique ID')['Claim Amount'].apply(\n",
    "        lambda x: x.std() / x.mean() if x.mean() > 0 else 0\n",
    "    ).reset_index()\n",
    "    cost_volatility.columns = ['Unique ID', 'claim_amount_cv']  # coefficient of variation\n",
    "    \n",
    "    features_list.append(severity)\n",
    "    features_list.append(cost_volatility)\n",
    "    \n",
    "    # === 6. TREND FEATURES (Increasing/Decreasing Claims) ===\n",
    "    def calculate_trend(group):\n",
    "        if len(group) < 2:\n",
    "            return 0\n",
    "        group = group.sort_values('Paid Date')\n",
    "        group['time_index'] = range(len(group))\n",
    "        correlation = group['time_index'].corr(group['Claim Amount'])\n",
    "        return correlation if not np.isnan(correlation) else 0\n",
    "    \n",
    "    trends = claims_df.groupby('Claimant Unique ID').apply(calculate_trend).reset_index()\n",
    "    trends.columns = ['Unique ID', 'claim_amount_trend']\n",
    "    \n",
    "    features_list.append(trends)\n",
    "    \n",
    "    # Merge all features\n",
    "    from functools import reduce\n",
    "    claims_features = reduce(lambda left, right: left.merge(right, on='Unique ID', how='outer'), features_list)\n",
    "    \n",
    "    # Fill NaN with 0 for members with no claims\n",
    "    claims_features = claims_features.fillna(0)\n",
    "    \n",
    "    return claims_features\n",
    "\n",
    "claims_features = create_claims_history_features(historical_claims, observation_date)\n",
    "claims_features.describe()\n",
    "print(f\"Claims features created: {claims_features.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4547518a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_claims_count</th>\n",
       "      <th>total_claim_amount</th>\n",
       "      <th>avg_claim_amount</th>\n",
       "      <th>median_claim_amount</th>\n",
       "      <th>std_claim_amount</th>\n",
       "      <th>min_claim_amount</th>\n",
       "      <th>max_claim_amount</th>\n",
       "      <th>claims_count_3m</th>\n",
       "      <th>total_amount_3m</th>\n",
       "      <th>claims_count_6m</th>\n",
       "      <th>...</th>\n",
       "      <th>days_since_first_claim</th>\n",
       "      <th>days_since_last_claim</th>\n",
       "      <th>claim_duration_days</th>\n",
       "      <th>avg_days_between_claims</th>\n",
       "      <th>avg_monthly_claim_frequency</th>\n",
       "      <th>std_monthly_claim_frequency</th>\n",
       "      <th>max_monthly_claims</th>\n",
       "      <th>high_cost_claims_count</th>\n",
       "      <th>claim_amount_cv</th>\n",
       "      <th>claim_amount_trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32857.000000</td>\n",
       "      <td>3.285700e+04</td>\n",
       "      <td>32857.000000</td>\n",
       "      <td>32857.000000</td>\n",
       "      <td>32857.000000</td>\n",
       "      <td>32857.000000</td>\n",
       "      <td>32857.000000</td>\n",
       "      <td>32857.000000</td>\n",
       "      <td>3.285700e+04</td>\n",
       "      <td>32857.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32857.000000</td>\n",
       "      <td>32857.000000</td>\n",
       "      <td>32857.00000</td>\n",
       "      <td>32857.000000</td>\n",
       "      <td>32857.000000</td>\n",
       "      <td>32857.000000</td>\n",
       "      <td>32857.000000</td>\n",
       "      <td>32857.000000</td>\n",
       "      <td>32857.000000</td>\n",
       "      <td>32857.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.506011</td>\n",
       "      <td>2.558287e+04</td>\n",
       "      <td>5477.329289</td>\n",
       "      <td>4278.385454</td>\n",
       "      <td>4080.006793</td>\n",
       "      <td>3116.627819</td>\n",
       "      <td>13236.995414</td>\n",
       "      <td>0.320541</td>\n",
       "      <td>1.818936e+03</td>\n",
       "      <td>0.620659</td>\n",
       "      <td>...</td>\n",
       "      <td>713.900387</td>\n",
       "      <td>623.257997</td>\n",
       "      <td>90.64239</td>\n",
       "      <td>38.289121</td>\n",
       "      <td>1.227895</td>\n",
       "      <td>0.176047</td>\n",
       "      <td>1.480659</td>\n",
       "      <td>1.126518</td>\n",
       "      <td>0.535784</td>\n",
       "      <td>-0.002648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>35.890613</td>\n",
       "      <td>2.142705e+05</td>\n",
       "      <td>16247.585935</td>\n",
       "      <td>15727.285264</td>\n",
       "      <td>12673.123009</td>\n",
       "      <td>14643.876577</td>\n",
       "      <td>33658.180062</td>\n",
       "      <td>4.926635</td>\n",
       "      <td>2.820913e+04</td>\n",
       "      <td>8.206342</td>\n",
       "      <td>...</td>\n",
       "      <td>418.583053</td>\n",
       "      <td>407.687561</td>\n",
       "      <td>122.78210</td>\n",
       "      <td>61.603699</td>\n",
       "      <td>2.541327</td>\n",
       "      <td>1.200852</td>\n",
       "      <td>3.767306</td>\n",
       "      <td>9.307321</td>\n",
       "      <td>0.753973</td>\n",
       "      <td>0.512540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.559000e+01</td>\n",
       "      <td>45.590000</td>\n",
       "      <td>45.590000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.660000</td>\n",
       "      <td>45.590000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.366600e+02</td>\n",
       "      <td>508.520000</td>\n",
       "      <td>461.710000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>232.240000</td>\n",
       "      <td>571.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>354.000000</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.809810e+03</td>\n",
       "      <td>1061.190000</td>\n",
       "      <td>814.615000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>451.340000</td>\n",
       "      <td>1386.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>707.000000</td>\n",
       "      <td>606.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.015042e+04</td>\n",
       "      <td>3547.030000</td>\n",
       "      <td>1660.740000</td>\n",
       "      <td>1338.369289</td>\n",
       "      <td>1013.670000</td>\n",
       "      <td>7696.810000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1064.000000</td>\n",
       "      <td>966.000000</td>\n",
       "      <td>182.00000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987087</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3974.000000</td>\n",
       "      <td>2.349523e+07</td>\n",
       "      <td>249923.290000</td>\n",
       "      <td>249923.290000</td>\n",
       "      <td>175583.926365</td>\n",
       "      <td>249923.290000</td>\n",
       "      <td>286107.430000</td>\n",
       "      <td>574.000000</td>\n",
       "      <td>2.905958e+06</td>\n",
       "      <td>690.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1591.000000</td>\n",
       "      <td>1578.000000</td>\n",
       "      <td>440.00000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>264.933333</td>\n",
       "      <td>111.126355</td>\n",
       "      <td>359.000000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>5.341215</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_claims_count  total_claim_amount  avg_claim_amount  \\\n",
       "count        32857.000000        3.285700e+04      32857.000000   \n",
       "mean             4.506011        2.558287e+04       5477.329289   \n",
       "std             35.890613        2.142705e+05      16247.585935   \n",
       "min              1.000000        4.559000e+01         45.590000   \n",
       "25%              1.000000        6.366600e+02        508.520000   \n",
       "50%              1.000000        1.809810e+03       1061.190000   \n",
       "75%              3.000000        1.015042e+04       3547.030000   \n",
       "max           3974.000000        2.349523e+07     249923.290000   \n",
       "\n",
       "       median_claim_amount  std_claim_amount  min_claim_amount  \\\n",
       "count         32857.000000      32857.000000      32857.000000   \n",
       "mean           4278.385454       4080.006793       3116.627819   \n",
       "std           15727.285264      12673.123009      14643.876577   \n",
       "min              45.590000          0.000000         40.660000   \n",
       "25%             461.710000          0.000000        232.240000   \n",
       "50%             814.615000          0.000000        451.340000   \n",
       "75%            1660.740000       1338.369289       1013.670000   \n",
       "max          249923.290000     175583.926365     249923.290000   \n",
       "\n",
       "       max_claim_amount  claims_count_3m  total_amount_3m  claims_count_6m  \\\n",
       "count      32857.000000     32857.000000     3.285700e+04     32857.000000   \n",
       "mean       13236.995414         0.320541     1.818936e+03         0.620659   \n",
       "std        33658.180062         4.926635     2.820913e+04         8.206342   \n",
       "min           45.590000         0.000000     0.000000e+00         0.000000   \n",
       "25%          571.400000         0.000000     0.000000e+00         0.000000   \n",
       "50%         1386.110000         0.000000     0.000000e+00         0.000000   \n",
       "75%         7696.810000         0.000000     0.000000e+00         0.000000   \n",
       "max       286107.430000       574.000000     2.905958e+06       690.000000   \n",
       "\n",
       "       ...  days_since_first_claim  days_since_last_claim  \\\n",
       "count  ...            32857.000000           32857.000000   \n",
       "mean   ...              713.900387             623.257997   \n",
       "std    ...              418.583053             407.687561   \n",
       "min    ...                1.000000               1.000000   \n",
       "25%    ...              354.000000             257.000000   \n",
       "50%    ...              707.000000             606.000000   \n",
       "75%    ...             1064.000000             966.000000   \n",
       "max    ...             1591.000000            1578.000000   \n",
       "\n",
       "       claim_duration_days  avg_days_between_claims  \\\n",
       "count          32857.00000             32857.000000   \n",
       "mean              90.64239                38.289121   \n",
       "std              122.78210                61.603699   \n",
       "min                0.00000                 0.000000   \n",
       "25%                0.00000                 0.000000   \n",
       "50%                0.00000                 0.000000   \n",
       "75%              182.00000                59.000000   \n",
       "max              440.00000               412.000000   \n",
       "\n",
       "       avg_monthly_claim_frequency  std_monthly_claim_frequency  \\\n",
       "count                 32857.000000                 32857.000000   \n",
       "mean                      1.227895                     0.176047   \n",
       "std                       2.541327                     1.200852   \n",
       "min                       1.000000                     0.000000   \n",
       "25%                       1.000000                     0.000000   \n",
       "50%                       1.000000                     0.000000   \n",
       "75%                       1.000000                     0.000000   \n",
       "max                     264.933333                   111.126355   \n",
       "\n",
       "       max_monthly_claims  high_cost_claims_count  claim_amount_cv  \\\n",
       "count        32857.000000            32857.000000     32857.000000   \n",
       "mean             1.480659                1.126518         0.535784   \n",
       "std              3.767306                9.307321         0.753973   \n",
       "min              1.000000                0.000000         0.000000   \n",
       "25%              1.000000                0.000000         0.000000   \n",
       "50%              1.000000                0.000000         0.000000   \n",
       "75%              1.000000                1.000000         0.987087   \n",
       "max            359.000000             1018.000000         5.341215   \n",
       "\n",
       "       claim_amount_trend  \n",
       "count        32857.000000  \n",
       "mean            -0.002648  \n",
       "std              0.512540  \n",
       "min             -1.000000  \n",
       "25%              0.000000  \n",
       "50%              0.000000  \n",
       "75%              0.000000  \n",
       "max              1.000000  \n",
       "\n",
       "[8 rows x 25 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claims_features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c949a96",
   "metadata": {},
   "source": [
    "#### 3.3 Treatment & Condition Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc764267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treatment/Condition features: 10\n"
     ]
    }
   ],
   "source": [
    "def create_treatment_condition_features(claims_df):\n",
    "    \"\"\"\n",
    "    Create features from Treatment Type and Condition information\n",
    "    \"\"\"\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    # === 1. TREATMENT TYPE FEATURES ===\n",
    "    treatment_features = claims_df.groupby('Claimant Unique ID').agg({\n",
    "        'Treatment Type': lambda x: x.nunique()  # unique treatment types\n",
    "    }).reset_index()\n",
    "    treatment_features.columns = ['Unique ID', 'unique_treatment_types']\n",
    "    \n",
    "    # Most frequent treatment\n",
    "    most_frequent_treatment = claims_df.groupby('Claimant Unique ID')['Treatment Type'].agg(\n",
    "        lambda x: x.mode()[0] if len(x.mode()) > 0 else 'Unknown'\n",
    "    ).reset_index()\n",
    "    most_frequent_treatment.columns = ['Unique ID', 'most_frequent_treatment']\n",
    "    \n",
    "    # Treatment diversity (entropy)\n",
    "    def calculate_entropy(series):\n",
    "        from scipy.stats import entropy\n",
    "        value_counts = series.value_counts(normalize=True)\n",
    "        return entropy(value_counts) if len(value_counts) > 1 else 0\n",
    "    \n",
    "    treatment_diversity = claims_df.groupby('Claimant Unique ID')['Treatment Type'].apply(\n",
    "        calculate_entropy\n",
    "    ).reset_index()\n",
    "    treatment_diversity.columns = ['Unique ID', 'treatment_diversity_entropy']\n",
    "    \n",
    "    features_list.extend([treatment_features, most_frequent_treatment, treatment_diversity])\n",
    "    \n",
    "    # === 2. CONDITION FEATURES ===\n",
    "    condition_features = claims_df.groupby('Claimant Unique ID').agg({\n",
    "        'Condition Code': 'nunique',\n",
    "        'Condition Category': 'nunique'\n",
    "    }).reset_index()\n",
    "    condition_features.columns = ['Unique ID', 'unique_condition_codes', 'unique_condition_categories']\n",
    "    \n",
    "    # Check for chronic conditions (repeat condition codes)\n",
    "    repeat_conditions = claims_df.groupby(['Claimant Unique ID', 'Condition Code']).size().reset_index(name='count')\n",
    "    chronic_indicator = repeat_conditions[repeat_conditions['count'] >= 3].groupby('Claimant Unique ID').size().reset_index()\n",
    "    chronic_indicator.columns = ['Unique ID', 'chronic_conditions_count']\n",
    "    \n",
    "    features_list.extend([condition_features, chronic_indicator])\n",
    "    \n",
    "    # === 3. HIGH-RISK CONDITION FLAGS ===\n",
    "    high_risk_conditions = ['CANCER', 'CARDIOVASCULAR', 'DIABETES', 'CHRONIC KIDNEY', 'MENTAL HEALTH']\n",
    "    \n",
    "    def has_high_risk_condition(condition_series):\n",
    "        condition_text = ' '.join(condition_series.astype(str).str.upper())\n",
    "        return any(condition in condition_text for condition in high_risk_conditions)\n",
    "    \n",
    "    high_risk_flags = claims_df.groupby('Claimant Unique ID')['Condition Category'].apply(\n",
    "        has_high_risk_condition\n",
    "    ).reset_index()\n",
    "    high_risk_flags.columns = ['Unique ID', 'has_high_risk_condition_history']\n",
    "    high_risk_flags['has_high_risk_condition_history'] = high_risk_flags['has_high_risk_condition_history'].astype(int)\n",
    "    \n",
    "    features_list.append(high_risk_flags)\n",
    "    \n",
    "    # === 4. TREATMENT LOCATION FEATURES ===\n",
    "    location_features = claims_df.groupby('Claimant Unique ID').agg({\n",
    "        'Treatment Location': lambda x: x.nunique()\n",
    "    }).reset_index()\n",
    "    location_features.columns = ['Unique ID', 'unique_treatment_locations']\n",
    "    \n",
    "    # Inpatient vs Outpatient ratio (if available)\n",
    "    # This depends on data structure\n",
    "    \n",
    "    features_list.append(location_features)\n",
    "    \n",
    "    # === 5. PROVIDER DIVERSITY ===\n",
    "    if 'Provider Type' in claims_df.columns:\n",
    "        provider_features = claims_df.groupby('Claimant Unique ID').agg({\n",
    "            'Provider Type': 'nunique'\n",
    "        }).reset_index()\n",
    "        provider_features.columns = ['Unique ID', 'unique_providers']\n",
    "        features_list.append(provider_features)\n",
    "    \n",
    "    # Merge all\n",
    "    from functools import reduce\n",
    "    treatment_condition_features = reduce(\n",
    "        lambda left, right: left.merge(right, on='Unique ID', how='outer'), \n",
    "        features_list\n",
    "    )\n",
    "    \n",
    "    treatment_condition_features = treatment_condition_features.fillna(0)\n",
    "    \n",
    "    return treatment_condition_features\n",
    "\n",
    "treatment_condition_features = create_treatment_condition_features(historical_claims)\n",
    "print(f\"Treatment/Condition features: {treatment_condition_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1049597b",
   "metadata": {},
   "source": [
    "#### 3.4 Risk Score Features (Composite Indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3edb120d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32857 entries, 0 to 32856\n",
      "Data columns (total 10 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   Unique ID                        32857 non-null  object \n",
      " 1   unique_treatment_types           32857 non-null  int64  \n",
      " 2   most_frequent_treatment          32857 non-null  object \n",
      " 3   treatment_diversity_entropy      32857 non-null  float64\n",
      " 4   unique_condition_codes           32857 non-null  int64  \n",
      " 5   unique_condition_categories      32857 non-null  int64  \n",
      " 6   chronic_conditions_count         32857 non-null  float64\n",
      " 7   has_high_risk_condition_history  32857 non-null  int32  \n",
      " 8   unique_treatment_locations       32857 non-null  int64  \n",
      " 9   unique_providers                 32857 non-null  int64  \n",
      "dtypes: float64(2), int32(1), int64(5), object(2)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "treatment_condition_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b0e3af14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32857 entries, 0 to 32856\n",
      "Data columns (total 26 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Unique ID                    32857 non-null  object \n",
      " 1   total_claims_count           32857 non-null  int64  \n",
      " 2   total_claim_amount           32857 non-null  float64\n",
      " 3   avg_claim_amount             32857 non-null  float64\n",
      " 4   median_claim_amount          32857 non-null  float64\n",
      " 5   std_claim_amount             32857 non-null  float64\n",
      " 6   min_claim_amount             32857 non-null  float64\n",
      " 7   max_claim_amount             32857 non-null  float64\n",
      " 8   claims_count_3m              32857 non-null  float64\n",
      " 9   total_amount_3m              32857 non-null  float64\n",
      " 10  claims_count_6m              32857 non-null  float64\n",
      " 11  total_amount_6m              32857 non-null  float64\n",
      " 12  claims_count_12m             32857 non-null  float64\n",
      " 13  total_amount_12m             32857 non-null  float64\n",
      " 14  claims_count_24m             32857 non-null  float64\n",
      " 15  total_amount_24m             32857 non-null  float64\n",
      " 16  days_since_first_claim       32857 non-null  int64  \n",
      " 17  days_since_last_claim        32857 non-null  int64  \n",
      " 18  claim_duration_days          32857 non-null  int64  \n",
      " 19  avg_days_between_claims      32857 non-null  float64\n",
      " 20  avg_monthly_claim_frequency  32857 non-null  float64\n",
      " 21  std_monthly_claim_frequency  32857 non-null  float64\n",
      " 22  max_monthly_claims           32857 non-null  int64  \n",
      " 23  high_cost_claims_count       32857 non-null  int32  \n",
      " 24  claim_amount_cv              32857 non-null  float64\n",
      " 25  claim_amount_trend           32857 non-null  float64\n",
      "dtypes: float64(19), int32(1), int64(5), object(1)\n",
      "memory usage: 6.4+ MB\n"
     ]
    }
   ],
   "source": [
    "claims_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "328cccaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120000 entries, 0 to 119999\n",
      "Data columns (total 16 columns):\n",
      " #   Column                         Non-Null Count   Dtype         \n",
      "---  ------                         --------------   -----         \n",
      " 0   Unique ID                      120000 non-null  object        \n",
      " 1   Year of Birth                  120000 non-null  int64         \n",
      " 2   age                            120000 non-null  int64         \n",
      " 3   age_group                      120000 non-null  category      \n",
      " 4   gender_Female                  120000 non-null  bool          \n",
      " 5   gender_Male                    120000 non-null  bool          \n",
      " 6   gender_Other                   120000 non-null  bool          \n",
      " 7   Original Date of Joining       120000 non-null  datetime64[ns]\n",
      " 8   membership_tenure_days         120000 non-null  int64         \n",
      " 9   membership_tenure_years        120000 non-null  float64       \n",
      " 10  Registration Status            120000 non-null  object        \n",
      " 11  is_active_member               120000 non-null  int32         \n",
      " 12  Scheme Category/ Section Name  120000 non-null  object        \n",
      " 13  contract_duration_days         120000 non-null  int64         \n",
      " 14  Lapse Date                     11963 non-null   object        \n",
      " 15  has_lapsed                     120000 non-null  int32         \n",
      "dtypes: bool(3), category(1), datetime64[ns](1), float64(1), int32(2), int64(4), object(4)\n",
      "memory usage: 10.5+ MB\n"
     ]
    }
   ],
   "source": [
    "demographic_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "82ae855d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_15380\\2188301879.py:23: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif pd.api.types.is_categorical_dtype(dtype):\n",
      "C:\\Temp\\ipykernel_15380\\2188301879.py:42: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  age_avg_claims = df.groupby('age_group')['claims_per_year'].transform('mean')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>composite_risk_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>120000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.377993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.708574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.327535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.200204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.200204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.200204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>79.944555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       composite_risk_score\n",
       "count         120000.000000\n",
       "mean               4.377993\n",
       "std                3.708574\n",
       "min                0.327535\n",
       "25%                3.200204\n",
       "50%                3.200204\n",
       "75%                3.200204\n",
       "max               79.944555"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_risk_score_features(claims_features, demographic_features):\n",
    "    \"\"\"\n",
    "    Create composite risk scores based on multiple factors\n",
    "    \"\"\"\n",
    "    \n",
    "    # Merge claims and demographic\n",
    "    merged = demographic_features.merge(claims_features, on='Unique ID', how='left')\n",
    "    merged = merged.merge(treatment_condition_features, on='Unique ID', how='left')\n",
    "\n",
    "    # Handle NaNs by dtype\n",
    "    for col in merged.columns:\n",
    "        dtype = merged[col].dtype\n",
    "\n",
    "        # Numeric columns\n",
    "        if pd.api.types.is_numeric_dtype(dtype):\n",
    "            merged[col] = merged[col].fillna(0)\n",
    "\n",
    "        # Boolean columns\n",
    "        elif pd.api.types.is_bool_dtype(dtype):\n",
    "            merged[col] = merged[col].fillna(False)\n",
    "\n",
    "        # Categorical columns\n",
    "        elif pd.api.types.is_categorical_dtype(dtype):\n",
    "            merged[col] = merged[col].cat.add_categories(['Unknown']).fillna('Unknown')\n",
    "\n",
    "        # Object columns (string, text)\n",
    "        elif pd.api.types.is_object_dtype(dtype):\n",
    "            merged[col] = merged[col].fillna('Unknown')\n",
    "\n",
    "        # Anything else â€” leave untouched\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    df = merged.copy()\n",
    "\n",
    "    # === 1. UTILIZATION RISK SCORE ===\n",
    "    # Normalized by age and membership tenure\n",
    "    df['claims_per_year'] = df['total_claims_count'] / (df['membership_tenure_years'] + 0.1)\n",
    "    df['cost_per_year'] = df['total_claim_amount'] / (df['membership_tenure_years'] + 0.1)\n",
    "    \n",
    "    # Age-adjusted utilization\n",
    "    age_avg_claims = df.groupby('age_group')['claims_per_year'].transform('mean')\n",
    "    df['utilization_vs_age_cohort'] = df['claims_per_year'] / (age_avg_claims + 1)\n",
    "    \n",
    "    # === 2. CLAIM ACCELERATION SCORE ===\n",
    "    # Are claims increasing over time?\n",
    "    df['claim_acceleration_score'] = (\n",
    "        (df['claims_count_6m'] / 6) / \n",
    "        ((df['claims_count_24m'] - df['claims_count_6m']) / 18 + 0.1)\n",
    "    )\n",
    "    df['claim_acceleration_score'] = df['claim_acceleration_score'].replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # === 3. RECENT ACTIVITY SCORE ===\n",
    "    df['recent_activity_score'] = (\n",
    "        0.5 * (df['claims_count_3m'] / (df['total_claims_count'] + 1)) +\n",
    "        0.3 * (df['total_amount_3m'] / (df['total_claim_amount'] + 1)) +\n",
    "        0.2 * (1 / (df['days_since_last_claim'] + 30))\n",
    "    )\n",
    "    \n",
    "    # === 4. SEVERITY RISK SCORE ===\n",
    "    df['severity_risk_score'] = (\n",
    "        0.4 * (df['high_cost_claims_count'] / (df['total_claims_count'] + 1)) +\n",
    "        0.3 * (df['max_claim_amount'] / (df['avg_claim_amount'] + 1)) +\n",
    "        0.3 * df['claim_amount_cv']\n",
    "    )\n",
    "    \n",
    "    # === 5. CHRONICITY SCORE ===\n",
    "    df['chronicity_score'] = (\n",
    "        0.5 * df['has_high_risk_condition_history'] +\n",
    "        0.3 * (df['chronic_conditions_count'] / (df['unique_condition_codes'] + 1)) +\n",
    "        0.2 * (df['avg_days_between_claims'] < 60).astype(int)\n",
    "    )\n",
    "    \n",
    "    # === 6. COMPOSITE AT-RISK SCORE (0-100) ===\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    risk_components = [\n",
    "        'utilization_vs_age_cohort',\n",
    "        'claim_acceleration_score', \n",
    "        'recent_activity_score',\n",
    "        'severity_risk_score',\n",
    "        'chronicity_score'\n",
    "    ]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    df[risk_components] = scaler.fit_transform(df[risk_components])\n",
    "    \n",
    "    df['composite_risk_score'] = (\n",
    "        0.25 * df['utilization_vs_age_cohort'] +\n",
    "        0.20 * df['claim_acceleration_score'] +\n",
    "        0.20 * df['recent_activity_score'] +\n",
    "        0.20 * df['severity_risk_score']+\n",
    "        0.15 * df['chronicity_score']\n",
    "    ) * 100\n",
    "    \n",
    "    return df\n",
    "\n",
    "risk_features = create_risk_score_features(claims_features, demographic_features)\n",
    "risk_features[['Unique ID', 'composite_risk_score']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "155f91de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>Year of Birth</th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "      <th>gender_Female</th>\n",
       "      <th>gender_Male</th>\n",
       "      <th>gender_Other</th>\n",
       "      <th>Original Date of Joining</th>\n",
       "      <th>membership_tenure_days</th>\n",
       "      <th>membership_tenure_years</th>\n",
       "      <th>Registration Status</th>\n",
       "      <th>is_active_member</th>\n",
       "      <th>Scheme Category/ Section Name</th>\n",
       "      <th>contract_duration_days</th>\n",
       "      <th>Lapse Date</th>\n",
       "      <th>has_lapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26963</th>\n",
       "      <td>MEM00011877-01</td>\n",
       "      <td>1972</td>\n",
       "      <td>52</td>\n",
       "      <td>45-54</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-08-18</td>\n",
       "      <td>2114</td>\n",
       "      <td>5.787817</td>\n",
       "      <td>Active</td>\n",
       "      <td>1</td>\n",
       "      <td>Platinum Plan</td>\n",
       "      <td>365</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70960</th>\n",
       "      <td>MEM00031345-04</td>\n",
       "      <td>2005</td>\n",
       "      <td>19</td>\n",
       "      <td>&lt;25</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-09-13</td>\n",
       "      <td>992</td>\n",
       "      <td>2.715948</td>\n",
       "      <td>Active</td>\n",
       "      <td>1</td>\n",
       "      <td>Enhanced Plan</td>\n",
       "      <td>365</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unique ID  Year of Birth  age age_group  gender_Female  \\\n",
       "26963  MEM00011877-01           1972   52     45-54           True   \n",
       "70960  MEM00031345-04           2005   19       <25           True   \n",
       "\n",
       "       gender_Male  gender_Other Original Date of Joining  \\\n",
       "26963        False         False               2018-08-18   \n",
       "70960        False         False               2021-09-13   \n",
       "\n",
       "       membership_tenure_days  membership_tenure_years Registration Status  \\\n",
       "26963                    2114                 5.787817              Active   \n",
       "70960                     992                 2.715948              Active   \n",
       "\n",
       "       is_active_member Scheme Category/ Section Name  contract_duration_days  \\\n",
       "26963                 1                 Platinum Plan                     365   \n",
       "70960                 1                 Enhanced Plan                     365   \n",
       "\n",
       "      Lapse Date  has_lapsed  \n",
       "26963       None           0  \n",
       "70960       None           0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_features.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d0c074c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>total_claims_count</th>\n",
       "      <th>total_claim_amount</th>\n",
       "      <th>avg_claim_amount</th>\n",
       "      <th>median_claim_amount</th>\n",
       "      <th>std_claim_amount</th>\n",
       "      <th>min_claim_amount</th>\n",
       "      <th>max_claim_amount</th>\n",
       "      <th>claims_count_3m</th>\n",
       "      <th>total_amount_3m</th>\n",
       "      <th>...</th>\n",
       "      <th>days_since_first_claim</th>\n",
       "      <th>days_since_last_claim</th>\n",
       "      <th>claim_duration_days</th>\n",
       "      <th>avg_days_between_claims</th>\n",
       "      <th>avg_monthly_claim_frequency</th>\n",
       "      <th>std_monthly_claim_frequency</th>\n",
       "      <th>max_monthly_claims</th>\n",
       "      <th>high_cost_claims_count</th>\n",
       "      <th>claim_amount_cv</th>\n",
       "      <th>claim_amount_trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24916</th>\n",
       "      <td>MEM00040160-03</td>\n",
       "      <td>5</td>\n",
       "      <td>2522.59</td>\n",
       "      <td>504.518</td>\n",
       "      <td>239.52</td>\n",
       "      <td>652.050033</td>\n",
       "      <td>149.84</td>\n",
       "      <td>1668.34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1171</td>\n",
       "      <td>885</td>\n",
       "      <td>286</td>\n",
       "      <td>71.5</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.292422</td>\n",
       "      <td>0.053381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10956</th>\n",
       "      <td>MEM00017639-01</td>\n",
       "      <td>3</td>\n",
       "      <td>6821.43</td>\n",
       "      <td>2273.810</td>\n",
       "      <td>1809.18</td>\n",
       "      <td>2331.637829</td>\n",
       "      <td>209.47</td>\n",
       "      <td>4802.78</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1809.18</td>\n",
       "      <td>...</td>\n",
       "      <td>436</td>\n",
       "      <td>73</td>\n",
       "      <td>363</td>\n",
       "      <td>181.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.025432</td>\n",
       "      <td>0.343044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unique ID  total_claims_count  total_claim_amount  \\\n",
       "24916  MEM00040160-03                   5             2522.59   \n",
       "10956  MEM00017639-01                   3             6821.43   \n",
       "\n",
       "       avg_claim_amount  median_claim_amount  std_claim_amount  \\\n",
       "24916           504.518               239.52        652.050033   \n",
       "10956          2273.810              1809.18       2331.637829   \n",
       "\n",
       "       min_claim_amount  max_claim_amount  claims_count_3m  total_amount_3m  \\\n",
       "24916            149.84           1668.34              0.0             0.00   \n",
       "10956            209.47           4802.78              1.0          1809.18   \n",
       "\n",
       "       ...  days_since_first_claim  days_since_last_claim  \\\n",
       "24916  ...                    1171                    885   \n",
       "10956  ...                     436                     73   \n",
       "\n",
       "       claim_duration_days  avg_days_between_claims  \\\n",
       "24916                  286                     71.5   \n",
       "10956                  363                    181.5   \n",
       "\n",
       "       avg_monthly_claim_frequency  std_monthly_claim_frequency  \\\n",
       "24916                         1.25                          0.5   \n",
       "10956                         1.00                          0.0   \n",
       "\n",
       "       max_monthly_claims  high_cost_claims_count  claim_amount_cv  \\\n",
       "24916                   2                       0         1.292422   \n",
       "10956                   1                       1         1.025432   \n",
       "\n",
       "       claim_amount_trend  \n",
       "24916            0.053381  \n",
       "10956            0.343044  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claims_features.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bf9152dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>unique_treatment_types</th>\n",
       "      <th>most_frequent_treatment</th>\n",
       "      <th>treatment_diversity_entropy</th>\n",
       "      <th>unique_condition_codes</th>\n",
       "      <th>unique_condition_categories</th>\n",
       "      <th>chronic_conditions_count</th>\n",
       "      <th>has_high_risk_condition_history</th>\n",
       "      <th>unique_treatment_locations</th>\n",
       "      <th>unique_providers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13257</th>\n",
       "      <td>MEM00021374-01</td>\n",
       "      <td>3</td>\n",
       "      <td>Specialist Consultation</td>\n",
       "      <td>1.039721</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19516</th>\n",
       "      <td>MEM00031370-02</td>\n",
       "      <td>1</td>\n",
       "      <td>Specialist Consultation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unique ID  unique_treatment_types  most_frequent_treatment  \\\n",
       "13257  MEM00021374-01                       3  Specialist Consultation   \n",
       "19516  MEM00031370-02                       1  Specialist Consultation   \n",
       "\n",
       "       treatment_diversity_entropy  unique_condition_codes  \\\n",
       "13257                     1.039721                       4   \n",
       "19516                     0.000000                       1   \n",
       "\n",
       "       unique_condition_categories  chronic_conditions_count  \\\n",
       "13257                            4                       0.0   \n",
       "19516                            1                       0.0   \n",
       "\n",
       "       has_high_risk_condition_history  unique_treatment_locations  \\\n",
       "13257                                0                           4   \n",
       "19516                                0                           1   \n",
       "\n",
       "       unique_providers  \n",
       "13257                 4  \n",
       "19516                 1  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treatment_condition_features.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "60766acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>future_total_claims</th>\n",
       "      <th>will_make_high_claim</th>\n",
       "      <th>will_develop_high_risk_condition</th>\n",
       "      <th>is_at_risk</th>\n",
       "      <th>risk_severity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34310</th>\n",
       "      <td>MEM00015158-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8225</th>\n",
       "      <td>MEM00003601-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unique ID  future_total_claims  will_make_high_claim  \\\n",
       "34310  MEM00015158-02                  0.0                     0   \n",
       "8225   MEM00003601-02                  0.0                     0   \n",
       "\n",
       "       will_develop_high_risk_condition  is_at_risk  risk_severity_score  \n",
       "34310                                 0           0                  0.0  \n",
       "8225                                  0           0                  0.0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aea3b820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>Year of Birth</th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "      <th>gender_Female</th>\n",
       "      <th>gender_Male</th>\n",
       "      <th>gender_Other</th>\n",
       "      <th>Original Date of Joining</th>\n",
       "      <th>membership_tenure_days</th>\n",
       "      <th>membership_tenure_years</th>\n",
       "      <th>...</th>\n",
       "      <th>unique_treatment_locations</th>\n",
       "      <th>unique_providers</th>\n",
       "      <th>claims_per_year</th>\n",
       "      <th>cost_per_year</th>\n",
       "      <th>utilization_vs_age_cohort</th>\n",
       "      <th>claim_acceleration_score</th>\n",
       "      <th>recent_activity_score</th>\n",
       "      <th>severity_risk_score</th>\n",
       "      <th>chronicity_score</th>\n",
       "      <th>composite_risk_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80590</th>\n",
       "      <td>MEM00035619-02</td>\n",
       "      <td>1980</td>\n",
       "      <td>44</td>\n",
       "      <td>35-44</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-08-13</td>\n",
       "      <td>2119</td>\n",
       "      <td>5.801506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.008259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202335</td>\n",
       "      <td>3.200204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45052</th>\n",
       "      <td>MEM00019871-01</td>\n",
       "      <td>1982</td>\n",
       "      <td>42</td>\n",
       "      <td>35-44</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-01-15</td>\n",
       "      <td>1233</td>\n",
       "      <td>3.375770</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.863118</td>\n",
       "      <td>11988.284539</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.00072</td>\n",
       "      <td>0.449680</td>\n",
       "      <td>0.064358</td>\n",
       "      <td>0.505837</td>\n",
       "      <td>17.907542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unique ID  Year of Birth  age age_group  gender_Female  \\\n",
       "80590  MEM00035619-02           1980   44     35-44          False   \n",
       "45052  MEM00019871-01           1982   42     35-44          False   \n",
       "\n",
       "       gender_Male  gender_Other Original Date of Joining  \\\n",
       "80590         True         False               2018-08-13   \n",
       "45052         True         False               2021-01-15   \n",
       "\n",
       "       membership_tenure_days  membership_tenure_years  ...  \\\n",
       "80590                    2119                 5.801506  ...   \n",
       "45052                    1233                 3.375770  ...   \n",
       "\n",
       "      unique_treatment_locations  unique_providers claims_per_year  \\\n",
       "80590                        0.0               0.0        0.000000   \n",
       "45052                        3.0               2.0        0.863118   \n",
       "\n",
       "       cost_per_year utilization_vs_age_cohort  claim_acceleration_score  \\\n",
       "80590       0.000000                  0.000000                   0.00000   \n",
       "45052   11988.284539                  0.000994                   0.00072   \n",
       "\n",
       "       recent_activity_score  severity_risk_score  chronicity_score  \\\n",
       "80590               0.008259             0.000000          0.202335   \n",
       "45052               0.449680             0.064358          0.505837   \n",
       "\n",
       "       composite_risk_score  \n",
       "80590              3.200204  \n",
       "45052             17.907542  \n",
       "\n",
       "[2 rows x 58 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_features.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e74c6897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unique ID', 'Year of Birth', 'age', 'age_group', 'gender_Female',\n",
       "       'gender_Male', 'gender_Other', 'Original Date of Joining',\n",
       "       'membership_tenure_days', 'membership_tenure_years',\n",
       "       'Registration Status', 'is_active_member',\n",
       "       'Scheme Category/ Section Name', 'contract_duration_days', 'Lapse Date',\n",
       "       'has_lapsed', 'total_claims_count', 'total_claim_amount',\n",
       "       'avg_claim_amount', 'median_claim_amount', 'std_claim_amount',\n",
       "       'min_claim_amount', 'max_claim_amount', 'claims_count_3m',\n",
       "       'total_amount_3m', 'claims_count_6m', 'total_amount_6m',\n",
       "       'claims_count_12m', 'total_amount_12m', 'claims_count_24m',\n",
       "       'total_amount_24m', 'days_since_first_claim', 'days_since_last_claim',\n",
       "       'claim_duration_days', 'avg_days_between_claims',\n",
       "       'avg_monthly_claim_frequency', 'std_monthly_claim_frequency',\n",
       "       'max_monthly_claims', 'high_cost_claims_count', 'claim_amount_cv',\n",
       "       'claim_amount_trend', 'unique_treatment_types',\n",
       "       'most_frequent_treatment', 'treatment_diversity_entropy',\n",
       "       'unique_condition_codes', 'unique_condition_categories',\n",
       "       'chronic_conditions_count', 'has_high_risk_condition_history',\n",
       "       'unique_treatment_locations', 'unique_providers', 'claims_per_year',\n",
       "       'cost_per_year', 'utilization_vs_age_cohort',\n",
       "       'claim_acceleration_score', 'recent_activity_score',\n",
       "       'severity_risk_score', 'chronicity_score', 'composite_risk_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593063a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57edf61b",
   "metadata": {},
   "source": [
    "### 3.5 Merge All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7778cbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master dataset shape: (120000, 63)\n",
      "Features: 58\n",
      "\n",
      "Feature Summary:\n",
      "float64           44\n",
      "object             5\n",
      "int32              5\n",
      "int64              4\n",
      "bool               3\n",
      "category           1\n",
      "datetime64[ns]     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values: 0\n"
     ]
    }
   ],
   "source": [
    "def create_master_feature_set(risk_features, targets):\n",
    "    \"\"\"\n",
    "    Combine all feature sets into final dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with risk_features\n",
    "    master_df = risk_features.copy()\n",
    "    \n",
    "    # Add targets\n",
    "    master_df = master_df.merge(targets, on='Unique ID', how='left')\n",
    "\n",
    "    \n",
    "    \n",
    "    print(f\"Master dataset shape: {master_df.shape}\")\n",
    "    print(f\"Features: {master_df.shape[1] - 5}\")  # excluding ID and targets\n",
    "    \n",
    "    return master_df\n",
    "\n",
    "master_df = create_master_feature_set(\n",
    "    risk_features,\n",
    "    targets\n",
    ")\n",
    "\n",
    "print(\"\\nFeature Summary:\")\n",
    "print(master_df.dtypes.value_counts())\n",
    "print(f\"\\nMissing values: {master_df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f3184e",
   "metadata": {},
   "source": [
    "#### STEP 4: Data Preprocessing\n",
    "##### 4.1 Handle Missing Values & Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1bfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before imputation:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "After preprocessing: (120000, 68)\n"
     ]
    }
   ],
   "source": [
    " def preprocess_features(df):\n",
    "    \"\"\"\n",
    "    Final preprocessing before modeling\n",
    "    \"\"\"\n",
    "    \n",
    "    # === 1. HANDLE MISSING VALUES ===\n",
    "    print(\"Missing values before imputation:\")\n",
    "    print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "    \n",
    "    # Numerical: median imputation\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    \n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df[numerical_cols] = imputer.fit_transform(df[numerical_cols])\n",
    "    \n",
    "    # === 2. HANDLE OUTLIERS (CAP AT 99TH PERCENTILE) ===\n",
    "    cost_columns = [col for col in df.columns if 'amount' in col.lower() or 'cost' in col.lower()]\n",
    "    \n",
    "    for col in cost_columns:\n",
    "        if df[col].dtype in [np.float64, np.int64]:\n",
    "            p99 = df[col].quantile(0.99)\n",
    "            df[col] = df[col].clip(upper=p99)\n",
    "    \n",
    "    # === 3. HANDLE INFINITE VALUES ===\n",
    "    df = df.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # === 4. ENCODE CATEGORICAL VARIABLES ===\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    # For high cardinality categoricals, use target encoding (do this during modeling)\n",
    "    # For low cardinality, use one-hot encoding\n",
    "    \n",
    "    low_cardinality = []\n",
    "    for col in categorical_cols:\n",
    "        if df[col].nunique() <= 10:\n",
    "            low_cardinality.append(col)\n",
    "    \n",
    "    df = pd.get_dummies(df, columns=low_cardinality, drop_first=True)\n",
    "    \n",
    "    print(f\"\\nAfter preprocessing: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "processed_df = preprocess_features(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cf485d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unique ID', 'Year of Birth', 'age', 'gender_Female', 'gender_Male',\n",
       "       'gender_Other', 'Original Date of Joining', 'membership_tenure_days',\n",
       "       'membership_tenure_years', 'is_active_member',\n",
       "       'Scheme Category/ Section Name', 'contract_duration_days', 'Lapse Date',\n",
       "       'has_lapsed', 'total_claims_count', 'total_claim_amount',\n",
       "       'avg_claim_amount', 'median_claim_amount', 'std_claim_amount',\n",
       "       'min_claim_amount', 'max_claim_amount', 'claims_count_3m',\n",
       "       'total_amount_3m', 'claims_count_6m', 'total_amount_6m',\n",
       "       'claims_count_12m', 'total_amount_12m', 'claims_count_24m',\n",
       "       'total_amount_24m', 'days_since_first_claim', 'days_since_last_claim',\n",
       "       'claim_duration_days', 'avg_days_between_claims',\n",
       "       'avg_monthly_claim_frequency', 'std_monthly_claim_frequency',\n",
       "       'max_monthly_claims', 'high_cost_claims_count', 'claim_amount_cv',\n",
       "       'claim_amount_trend', 'unique_treatment_types',\n",
       "       'most_frequent_treatment', 'treatment_diversity_entropy',\n",
       "       'unique_condition_codes', 'unique_condition_categories',\n",
       "       'chronic_conditions_count', 'has_high_risk_condition_history',\n",
       "       'unique_treatment_locations', 'unique_providers', 'claims_per_year',\n",
       "       'cost_per_year', 'utilization_vs_age_cohort',\n",
       "       'claim_acceleration_score', 'recent_activity_score',\n",
       "       'severity_risk_score', 'chronicity_score', 'composite_risk_score',\n",
       "       'future_total_claims', 'will_make_high_claim',\n",
       "       'will_develop_high_risk_condition', 'is_at_risk', 'risk_severity_score',\n",
       "       'age_group_25-34', 'age_group_35-44', 'age_group_45-54',\n",
       "       'age_group_55-64', 'age_group_65+', 'age_group_Unknown',\n",
       "       'Registration Status_Lapsed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02f01b",
   "metadata": {},
   "source": [
    "# 4.2 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "85548d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_15380\\691388711.py:30: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif pd.api.types.is_object_dtype(X[col]) or pd.api.types.is_categorical_dtype(X[col]):\n",
      "C:\\Temp\\ipykernel_15380\\691388711.py:30: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif pd.api.types.is_object_dtype(X[col]) or pd.api.types.is_categorical_dtype(X[col]):\n",
      "C:\\Temp\\ipykernel_15380\\691388711.py:30: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif pd.api.types.is_object_dtype(X[col]) or pd.api.types.is_categorical_dtype(X[col]):\n",
      "C:\\Temp\\ipykernel_15380\\691388711.py:30: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif pd.api.types.is_object_dtype(X[col]) or pd.api.types.is_categorical_dtype(X[col]):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping non-numeric/unencoded columns: ['Original Date of Joining', 'Lapse Date']\n",
      "âœ… Features after variance filter: 55\n",
      "\n",
      "ðŸ“Š Top 20 features by F-score:\n",
      "                        feature        score\n",
      "18              total_amount_3m  9040.486591\n",
      "46        recent_activity_score  7524.030475\n",
      "20              total_amount_6m  7227.962512\n",
      "48         composite_risk_score  4844.308412\n",
      "22             total_amount_12m  3605.454087\n",
      "17              claims_count_3m  2160.144243\n",
      "19              claims_count_6m  2136.104755\n",
      "6       membership_tenure_years  2027.724421\n",
      "5        membership_tenure_days  2027.724421\n",
      "24             total_amount_24m  1479.041386\n",
      "45                cost_per_year  1215.600697\n",
      "21             claims_count_12m  1173.332588\n",
      "42   unique_treatment_locations   916.852701\n",
      "38       unique_condition_codes   894.300686\n",
      "35       unique_treatment_types   873.148338\n",
      "39  unique_condition_categories   816.538978\n",
      "43             unique_providers   741.996197\n",
      "37  treatment_diversity_entropy   728.625298\n",
      "32       high_cost_claims_count   649.442701\n",
      "33              claim_amount_cv   597.131700\n",
      "\n",
      "ðŸŒ³ Top 20 features by Random Forest importance:\n",
      "                          feature  importance\n",
      "5          membership_tenure_days    0.220801\n",
      "6         membership_tenure_years    0.219891\n",
      "8   Scheme Category/ Section Name    0.090610\n",
      "1                             age    0.043553\n",
      "0                   Year of Birth    0.043544\n",
      "46          recent_activity_score    0.030881\n",
      "48           composite_risk_score    0.021883\n",
      "25         days_since_first_claim    0.021604\n",
      "18                total_amount_3m    0.019272\n",
      "17                claims_count_3m    0.019000\n",
      "26          days_since_last_claim    0.017798\n",
      "20                total_amount_6m    0.017797\n",
      "19                claims_count_6m    0.016671\n",
      "44                claims_per_year    0.016164\n",
      "15               min_claim_amount    0.011660\n",
      "22               total_amount_12m    0.011098\n",
      "16               max_claim_amount    0.010967\n",
      "12               avg_claim_amount    0.010921\n",
      "27            claim_duration_days    0.010828\n",
      "45                  cost_per_year    0.010665\n",
      "\n",
      "ðŸ Final selected features: 54\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def select_important_features(df, target_col='is_at_risk', top_k=50):\n",
    "    \"\"\"\n",
    "    Select most important features using variance filtering,\n",
    "    univariate tests, and tree-based importance.\n",
    "    Automatically handles datetime, bool, and categorical columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Separate features and target\n",
    "    feature_cols = [col for col in df.columns if col not in [\n",
    "        'Unique ID', 'is_at_risk', 'will_make_high_claim', \n",
    "        'will_develop_high_risk_condition', 'future_total_claims', 'risk_severity_score'\n",
    "    ]]\n",
    "    \n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col]\n",
    "\n",
    "    # --- 2. Drop or encode unsupported dtypes\n",
    "    # Convert bool â†’ int, drop datetime/object/categorical\n",
    "    drop_cols = []\n",
    "    for col in X.columns:\n",
    "        if np.issubdtype(X[col].dtype, np.datetime64):\n",
    "            drop_cols.append(col)\n",
    "        elif pd.api.types.is_bool_dtype(X[col]):\n",
    "            X[col] = X[col].astype(int)\n",
    "        elif pd.api.types.is_object_dtype(X[col]) or pd.api.types.is_categorical_dtype(X[col]):\n",
    "            # Convert to category codes (or drop if too many unique)\n",
    "            if X[col].nunique() < 50:\n",
    "                X[col] = X[col].astype('category').cat.codes\n",
    "            else:\n",
    "                drop_cols.append(col)\n",
    "\n",
    "    if drop_cols:\n",
    "        print(f\"Dropping non-numeric/unencoded columns: {drop_cols}\")\n",
    "        X = X.drop(columns=drop_cols)\n",
    "    \n",
    "    # Replace any remaining NaNs with 0 (safe for numeric)\n",
    "    X = X.fillna(0)\n",
    "\n",
    "    # --- 3. Remove low variance features\n",
    "    selector = VarianceThreshold(threshold=0.01)\n",
    "    X_high_var = selector.fit_transform(X)\n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    \n",
    "    print(f\"âœ… Features after variance filter: {len(selected_features)}\")\n",
    "\n",
    "    # --- 4. Univariate feature selection (ANOVA F-test)\n",
    "    X_selected = X[selected_features]\n",
    "    selector = SelectKBest(f_classif, k=min(top_k, len(selected_features)))\n",
    "    selector.fit(X_selected, y)\n",
    "    \n",
    "    feature_scores = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'score': selector.scores_\n",
    "    }).sort_values('score', ascending=False)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Top 20 features by F-score:\")\n",
    "    print(feature_scores.head(20))\n",
    "\n",
    "    # --- 5. Random Forest feature importance\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_selected, y)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nðŸŒ³ Top 20 features by Random Forest importance:\")\n",
    "    print(feature_importance.head(20))\n",
    "\n",
    "    # --- 6. Combine top features from both methods\n",
    "    top_features_f = set(feature_scores.head(top_k)['feature'])\n",
    "    top_features_rf = set(feature_importance.head(top_k)['feature'])\n",
    "    final_features = list(top_features_f.union(top_features_rf))\n",
    "    \n",
    "    print(f\"\\nðŸ Final selected features: {len(final_features)}\")\n",
    "    \n",
    "    return final_features, feature_importance\n",
    "\n",
    "\n",
    "\n",
    "selected_features, feature_importance = select_important_features(processed_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b2393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6a557e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c1fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
