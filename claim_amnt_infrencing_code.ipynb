{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6216f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "298a69da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_claims_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Mirror the training-time claims cleaning pipeline.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # 1. Standardise column names\n",
    "    df_clean.columns = df_clean.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "    # 2. Parse date columns\n",
    "    date_columns = [\n",
    "        'paid_date', 'incurred_date', 'admission_date', 'discharge_date',\n",
    "        'contract_start_date', 'contract_end_date'\n",
    "    ]\n",
    "    for col in date_columns:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n",
    "\n",
    "    # 3. Remove invalid rows\n",
    "    if 'claim_amount' in df_clean.columns:\n",
    "        df_clean = df_clean[df_clean['claim_amount'] > 0]\n",
    "    if 'paid_date' in df_clean.columns:\n",
    "        df_clean = df_clean[df_clean['paid_date'] <= pd.Timestamp.now()]\n",
    "    if 'incurred_date' in df_clean.columns and 'paid_date' in df_clean.columns:\n",
    "        df_clean = df_clean[df_clean['incurred_date'] <= df_clean['paid_date']]\n",
    "\n",
    "    # 4. Fill missing categorical fields\n",
    "    if 'treatment_type' in df_clean.columns:\n",
    "        df_clean['treatment_type'].fillna('Unknown', inplace=True)\n",
    "    if 'provider_type' in df_clean.columns:\n",
    "        df_clean['provider_type'].fillna('Unknown', inplace=True)\n",
    "    if 'treatment_location' in df_clean.columns:\n",
    "        mode_val = df_clean['treatment_location'].mode()\n",
    "        df_clean['treatment_location'].fillna(mode_val[0] if not mode_val.empty else 'Unknown', inplace=True)\n",
    "    if 'condition_category' in df_clean.columns:\n",
    "        df_clean['condition_category'].fillna('Unspecified', inplace=True)\n",
    "    if 'admission_date' in df_clean.columns and 'discharge_date' in df_clean.columns:\n",
    "        mask = df_clean['admission_date'].notna() & df_clean['discharge_date'].isna()\n",
    "        df_clean.loc[mask, 'discharge_date'] = df_clean.loc[mask, 'admission_date']\n",
    "\n",
    "    # 5. Standardise categoricals\n",
    "    if 'claimant_gender' in df_clean.columns:\n",
    "        gender_mapping = {\n",
    "            'M': 'Male', 'F': 'Female', 'O': 'Other',\n",
    "            'male': 'Male', 'female': 'Female', 'other': 'Other',\n",
    "            'MALE': 'Male', 'FEMALE': 'Female', 'OTHER': 'Other'\n",
    "        }\n",
    "        df_clean['claimant_gender'] = df_clean['claimant_gender'].map(\n",
    "            lambda x: gender_mapping.get(x, x) if pd.notna(x) else 'Unknown'\n",
    "        )\n",
    "    if 'status_of_member' in df_clean.columns:\n",
    "        df_clean['status_of_member'] = df_clean['status_of_member'].astype(str).str.strip().str.title()\n",
    "\n",
    "    # 6. Flag outliers (retain them for modelling)\n",
    "    if 'claim_amount' in df_clean.columns:\n",
    "        q1 = df_clean['claim_amount'].quantile(0.25)\n",
    "        q3 = df_clean['claim_amount'].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        df_clean['is_outlier'] = (\n",
    "            (df_clean['claim_amount'] < (q1 - 3 * iqr)) |\n",
    "            (df_clean['claim_amount'] > (q3 + 3 * iqr))\n",
    "        ).astype(int)\n",
    "\n",
    "    # 7. Ensure claim_id exists\n",
    "    if 'claim_id' not in df_clean.columns:\n",
    "        df_clean['claim_id'] = range(1, len(df_clean) + 1)\n",
    "\n",
    "    # 8. Drop duplicate claims\n",
    "    df_clean = df_clean.drop_duplicates(subset=['claim_id'], keep='first')\n",
    "\n",
    "    print(f\"Claims Cleaning Summary -> original: {len(df):,}, cleaned: {len(df_clean):,}\")\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def clean_membership_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Mirror the training-time membership cleaning.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # 1. Standardise column names\n",
    "    df_clean.columns = df_clean.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "    # 2. Parse date columns\n",
    "    date_columns = [\n",
    "        'contract_staart_date', 'contract_end_date', 'original_date_of_joining',\n",
    "        'sscheme_policy_joining_date', 'lapse_date'\n",
    "    ]\n",
    "    for col in date_columns:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n",
    "\n",
    "    # 3. Filter invalid birth years\n",
    "    current_year = datetime.now().year\n",
    "    if 'year_of_birth' in df_clean.columns:\n",
    "        df_clean = df_clean[(df_clean['year_of_birth'] >= 1920) & (df_clean['year_of_birth'] <= current_year)]\n",
    "    if 'claimant_year_of_birth' in df_clean.columns:\n",
    "        df_clean = df_clean[(df_clean['claimant_year_of_birth'] >= 1920) & (df_clean['claimant_year_of_birth'] <= current_year)]\n",
    "\n",
    "    # 4. Standardise gender\n",
    "    if 'gender' in df_clean.columns:\n",
    "        gender_mapping = {\n",
    "            'M': 'Male', 'F': 'Female', 'O': 'Other',\n",
    "            'male': 'Male', 'female': 'Female', 'other': 'Other',\n",
    "            'MALE': 'Male', 'FEMALE': 'Female', 'OTHER': 'Other'\n",
    "        }\n",
    "        df_clean['gender'] = df_clean['gender'].map(\n",
    "            lambda x: gender_mapping.get(x, x) if pd.notna(x) else 'Unknown'\n",
    "        )\n",
    "\n",
    "    # 5. Status fields\n",
    "    if 'status_of_member' in df_clean.columns:\n",
    "        df_clean['status_of_member'] = df_clean['status_of_member'].astype(str).str.strip().str.title().fillna('Active')\n",
    "    if 'registration_status' in df_clean.columns:\n",
    "        df_clean['registration_status'] = df_clean['registration_status'].astype(str).str.strip().str.title().fillna('Registered')\n",
    "    if 'status_of_registration' in df_clean.columns:\n",
    "        df_clean['status_of_registration'] = df_clean['status_of_registration'].astype(str).str.strip().str.title()\n",
    "\n",
    "    # 6. Postcode cleaning\n",
    "    if 'short_post_code' in df_clean.columns:\n",
    "        df_clean['short_post_code'] = df_clean['short_post_code'].astype(str).str.strip().str.upper()\n",
    "    if 'short_post_code_of_member' in df_clean.columns:\n",
    "        df_clean['short_post_code_of_member'] = df_clean['short_post_code_of_member'].astype(str).str.strip().str.upper()\n",
    "\n",
    "    # 7. Drop duplicates on unique ID\n",
    "    if 'unique_id' in df_clean.columns:\n",
    "        df_clean = df_clean.drop_duplicates(subset=['unique_id'], keep='first')\n",
    "    elif 'claimant_unique_id' in df_clean.columns:\n",
    "        df_clean = df_clean.drop_duplicates(subset=['claimant_unique_id'], keep='first')\n",
    "\n",
    "    print(f\"Membership Cleaning Summary -> original: {len(df):,}, cleaned: {len(df_clean):,}\")\n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "edfa4563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_qcut(series: pd.Series, q: int = 5, labels=None):\n",
    "    \"\"\"Safely apply qcut even when unique values < q.\"\"\"\n",
    "    if labels is None:\n",
    "        labels = [1, 2, 3, 4, 5]\n",
    "    series = series.fillna(0)\n",
    "    unique_vals = series.nunique()\n",
    "    if unique_vals == 0:\n",
    "        return pd.Series([1] * len(series), index=series.index)\n",
    "    q_eff = min(q, unique_vals)\n",
    "    labels_eff = labels[:q_eff]\n",
    "    try:\n",
    "        return pd.qcut(series.rank(method='first'), q=q_eff, labels=labels_eff, duplicates='drop')\n",
    "    except ValueError:\n",
    "        return pd.Series([labels_eff[0]] * len(series), index=series.index)\n",
    "\n",
    "\n",
    "def create_member_features(members_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Copy of the training-time member feature builder.\"\"\"\n",
    "    df = members_df.copy()\n",
    "    current_date = pd.Timestamp.now()\n",
    "\n",
    "    # Demographics\n",
    "    if 'year_of_birth' in df.columns:\n",
    "        df['current_age'] = current_date.year - df['year_of_birth']\n",
    "        df['age_band'] = pd.cut(df['current_age'], bins=[0, 18, 30, 40, 50, 60, 70, 120],\n",
    "                                labels=['0-18', '19-30', '31-40', '41-50', '51-60', '61-70', '70+'])\n",
    "        df['age_risk_category'] = pd.cut(df['current_age'], bins=[0, 35, 55, 120],\n",
    "                                         labels=['Low', 'Medium', 'High'])\n",
    "    if 'gender' in df.columns:\n",
    "        df['gender_encoded'] = df['gender'].map({'Male': 1, 'Female': 2, 'Other': 3, 'Unknown': 0})\n",
    "\n",
    "    # Tenure metrics\n",
    "    if 'original_date_of_joining' in df.columns:\n",
    "        df['membership_tenure_days'] = (current_date - df['original_date_of_joining']).dt.days\n",
    "        df['membership_tenure_years'] = df['membership_tenure_days'] / 365.25\n",
    "        df['membership_tenure_months'] = df['membership_tenure_days'] / 30.44\n",
    "        df['tenure_category'] = pd.cut(df['membership_tenure_years'],\n",
    "                                       bins=[-np.inf, 1, 3, 5, 10, np.inf],\n",
    "                                       labels=['New(<1yr)', 'Recent(1-3yr)', 'Established(3-5yr)',\n",
    "                                               'Mature(5-10yr)', 'Long-term(10+yr)'])\n",
    "    if 'sscheme_policy_joining_date' in df.columns:\n",
    "        df['scheme_tenure_days'] = (current_date - df['sscheme_policy_joining_date']).dt.days\n",
    "        df['scheme_tenure_months'] = df['scheme_tenure_days'] / 30.44\n",
    "\n",
    "    # Contract information\n",
    "    if 'contract_staart_date' in df.columns and 'contract_end_date' in df.columns:\n",
    "        df['contract_duration_days'] = (df['contract_end_date'] - df['contract_staart_date']).dt.days\n",
    "        df['contract_remaining_days'] = (df['contract_end_date'] - current_date).dt.days\n",
    "        df['contract_elapsed_ratio'] = (\n",
    "            (current_date - df['contract_staart_date']).dt.days / df['contract_duration_days']\n",
    "        ).clip(0, 1)\n",
    "        df['near_renewal'] = (df['contract_remaining_days'] <= 90).astype(int)\n",
    "        df['contract_expired'] = (df['contract_remaining_days'] < 0).astype(int)\n",
    "        df['contract_year'] = df['contract_staart_date'].dt.year\n",
    "        df['contract_start_month'] = df['contract_staart_date'].dt.month\n",
    "        df['contract_start_quarter'] = df['contract_staart_date'].dt.quarter\n",
    "\n",
    "    # Status features\n",
    "    if 'status_of_member' in df.columns:\n",
    "        df['is_active_member'] = (df['status_of_member'] == 'Active').astype(int)\n",
    "    if 'registration_status' in df.columns:\n",
    "        df['is_registered'] = (df['registration_status'] == 'Registered').astype(int)\n",
    "\n",
    "    # Geography\n",
    "    if 'short_post_code_of_member' in df.columns:\n",
    "        df['postcode_area'] = df['short_post_code_of_member'].str[:2]\n",
    "\n",
    "    # Derived indicators\n",
    "    if 'membership_tenure_years' in df.columns:\n",
    "        df['tenure_risk_bucket'] = pd.cut(df['membership_tenure_years'],\n",
    "                                          bins=[-np.inf, 1, 3, 5, 10, np.inf],\n",
    "                                          labels=[1, 2, 3, 4, 5])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "95574384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_claims_features(claims_df: pd.DataFrame, reference_date: pd.Timestamp | None = None) -> pd.DataFrame:\n",
    "    \"\"\"Copy of training-time claim level feature engineering.\"\"\"\n",
    "    df = claims_df.copy()\n",
    "    if reference_date is None:\n",
    "        reference_date = pd.Timestamp.now()\n",
    "\n",
    "    # Claim amount transformations\n",
    "    if 'claim_amount' in df.columns:\n",
    "        df['claim_amount_log'] = np.log1p(df['claim_amount'])\n",
    "        df['claim_amount_band'] = pd.cut(\n",
    "            df['claim_amount'],\n",
    "            bins=[0, 500, 1_000, 2_500, 5_000, 10_000, np.inf],\n",
    "            labels=['Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical']\n",
    "        )\n",
    "        high_cost_threshold = df['claim_amount'].quantile(0.90)\n",
    "        df['is_high_cost_claim'] = (df['claim_amount'] >= high_cost_threshold).astype(int)\n",
    "        catastrophic_threshold = df['claim_amount'].quantile(0.99)\n",
    "        df['is_catastrophic_claim'] = (df['claim_amount'] >= catastrophic_threshold).astype(int)\n",
    "    if 'amount_paid' in df.columns:\n",
    "        df['amount_paid_log'] = np.log1p(df['amount_paid'])\n",
    "\n",
    "    # Paid date features\n",
    "    if 'paid_date' in df.columns:\n",
    "        df['paid_year'] = df['paid_date'].dt.year\n",
    "        df['paid_month'] = df['paid_date'].dt.month\n",
    "        df['paid_quarter'] = df['paid_date'].dt.quarter\n",
    "        df['paid_day_of_week'] = df['paid_date'].dt.dayofweek\n",
    "        df['paid_week_of_year'] = df['paid_date'].dt.isocalendar().week\n",
    "        df['paid_day_of_year'] = df['paid_date'].dt.dayofyear\n",
    "        df['paid_on_weekend'] = (df['paid_day_of_week'] >= 5).astype(int)\n",
    "        df['paid_in_winter'] = df['paid_month'].isin([12, 1, 2]).astype(int)\n",
    "        df['paid_in_flu_season'] = df['paid_month'].isin([10, 11, 12, 1, 2, 3]).astype(int)\n",
    "        df['days_since_paid'] = (reference_date - df['paid_date']).dt.days\n",
    "        df['months_since_paid'] = df['days_since_paid'] / 30.44\n",
    "        df['claim_recency'] = pd.cut(\n",
    "            df['days_since_paid'],\n",
    "            bins=[-np.inf, 30, 90, 180, 365, np.inf],\n",
    "            labels=['Very Recent', 'Recent', 'Moderate', 'Old', 'Very Old']\n",
    "        )\n",
    "\n",
    "    # Incurred date features\n",
    "    if 'incurred_date' in df.columns:\n",
    "        df['incurred_year'] = df['incurred_date'].dt.year\n",
    "        df['incurred_month'] = df['incurred_date'].dt.month\n",
    "        df['incurred_quarter'] = df['incurred_date'].dt.quarter\n",
    "        df['incurred_day_of_week'] = df['incurred_date'].dt.dayofweek\n",
    "        df['incurred_week_of_year'] = df['incurred_date'].dt.isocalendar().week\n",
    "        df['days_since_incurred'] = (reference_date - df['incurred_date']).dt.days\n",
    "\n",
    "    # Processing time\n",
    "    if 'incurred_date' in df.columns and 'paid_date' in df.columns:\n",
    "        df['processing_time_days'] = (df['paid_date'] - df['incurred_date']).dt.days\n",
    "        df['processing_time_weeks'] = df['processing_time_days'] / 7\n",
    "        df['processing_speed'] = pd.cut(\n",
    "            df['processing_time_days'],\n",
    "            bins=[-np.inf, 7, 30, 90, np.inf],\n",
    "            labels=['Fast', 'Normal', 'Slow', 'Very Slow']\n",
    "        )\n",
    "        df['is_delayed_claim'] = (df['processing_time_days'] > 90).astype(int)\n",
    "\n",
    "    # Admission / length of stay\n",
    "    if 'admission_date' in df.columns and 'discharge_date' in df.columns:\n",
    "        df['length_of_stay_days'] = (df['discharge_date'] - df['admission_date']).dt.days\n",
    "        df['length_of_stay_category'] = pd.cut(\n",
    "            df['length_of_stay_days'],\n",
    "            bins=[-np.inf, 0, 1, 3, 7, 14, np.inf],\n",
    "            labels=['Day_Case', 'Overnight', 'Short', 'Medium', 'Long', 'Extended']\n",
    "        )\n",
    "        df['is_inpatient'] = (df['length_of_stay_days'] > 0).astype(int)\n",
    "        df['admission_day_of_week'] = df['admission_date'].dt.dayofweek\n",
    "        df['admitted_on_weekend'] = (df['admission_day_of_week'] >= 5).astype(int)\n",
    "        df['likely_emergency'] = df['admitted_on_weekend']\n",
    "\n",
    "    # Condition & treatment descriptors\n",
    "    if 'condition_category' in df.columns:\n",
    "        df['condition_category_encoded'] = df['condition_category'].astype('category')\n",
    "        chronic_keywords = ['diabetes', 'cancer', 'cardiac', 'renal', 'chronic']\n",
    "        df['is_chronic_condition'] = df['condition_category'].str.lower().str.contains(\n",
    "            '|'.join(chronic_keywords), na=False\n",
    "        ).astype(int)\n",
    "    if 'condition_code' in df.columns:\n",
    "        df['condition_code_category'] = df['condition_code'].astype(str).str[:3]\n",
    "    if 'treatment_type' in df.columns:\n",
    "        df['treatment_category'] = df['treatment_type'].astype('category')\n",
    "        major_keywords = ['surgery', 'operation', 'transplant', 'bypass']\n",
    "        df['is_major_treatment'] = df['treatment_type'].str.lower().str.contains(\n",
    "            '|'.join(major_keywords), na=False\n",
    "        ).astype(int)\n",
    "    if 'ancillary_service_type' in df.columns:\n",
    "        df['has_ancillary_service'] = df['ancillary_service_type'].notna().astype(int)\n",
    "\n",
    "    # Provider/location patterns\n",
    "    if 'provider_type' in df.columns:\n",
    "        df['provider_category'] = df['provider_type'].astype('category')\n",
    "    if 'treatment_location' in df.columns:\n",
    "        df['treatment_location_category'] = df['treatment_location'].astype('category')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e4fd20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aggregated_member_claim_features(\n",
    "    claims_df: pd.DataFrame,\n",
    "    members_df: pd.DataFrame,\n",
    "    observation_date: pd.Timestamp | None = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Aggregate claims history onto members exactly as in training.\"\"\"\n",
    "    if observation_date is None:\n",
    "        observation_date = pd.Timestamp.now()\n",
    "\n",
    "    member_id_col = 'claimant_unique_id' if 'claimant_unique_id' in claims_df.columns else 'unique_id'\n",
    "    historical_claims = claims_df[claims_df.get('paid_date', observation_date) <= observation_date].copy()\n",
    "\n",
    "    # Member level aggregations\n",
    "    member_agg = historical_claims.groupby(member_id_col).agg({\n",
    "        'claim_id': 'count',\n",
    "        'claim_amount': ['sum', 'mean', 'median', 'std', 'max', 'min']\n",
    "    }).reset_index()\n",
    "    member_agg.columns = [member_id_col, 'total_claims', 'total_claim_amount', 'avg_claim_amount',\n",
    "                          'median_claim_amount', 'std_claim_amount', 'max_claim_amount', 'min_claim_amount']\n",
    "\n",
    "    periods = {\n",
    "        '12m': 365,\n",
    "        '6m': 182,\n",
    "        '3m': 90\n",
    "    }\n",
    "    member_period_aggs = []\n",
    "    for label, days in periods.items():\n",
    "        cutoff = observation_date - pd.Timedelta(days=days)\n",
    "        period_claims = historical_claims[historical_claims['paid_date'] >= cutoff]\n",
    "        period_agg = period_claims.groupby(member_id_col).agg({\n",
    "            'claim_id': 'count',\n",
    "            'claim_amount': 'sum'\n",
    "        }).reset_index()\n",
    "        period_agg.columns = [member_id_col, f'claim_amount_{label}_count', f'claim_amount_{label}_sum']\n",
    "        member_period_aggs.append(period_agg)\n",
    "\n",
    "    # High cost aggregates\n",
    "    if 'claim_amount' in historical_claims.columns:\n",
    "        high_cost_threshold = historical_claims['claim_amount'].quantile(0.90)\n",
    "        high_cost_claims = historical_claims[historical_claims['claim_amount'] >= high_cost_threshold]\n",
    "        high_cost_agg = high_cost_claims.groupby(member_id_col).agg({\n",
    "            'claim_amount': ['count', 'sum', 'mean']\n",
    "        }).reset_index()\n",
    "        high_cost_agg.columns = [member_id_col, 'claim_amount_high_cost_count',\n",
    "                                 'claim_amount_high_cost_sum', 'claim_amount_high_cost_mean']\n",
    "    else:\n",
    "        high_cost_agg = pd.DataFrame({member_id_col: []})\n",
    "\n",
    "    claims_sorted = historical_claims.sort_values([member_id_col, 'paid_date'])\n",
    "    claims_sorted['days_since_last_claim'] = claims_sorted.groupby(member_id_col)['paid_date'].diff().dt.days\n",
    "    claim_frequency = claims_sorted.groupby(member_id_col)['days_since_last_claim'].agg([\n",
    "        ('avg_days_between_claims', 'mean'),\n",
    "        ('median_days_between_claims', 'median'),\n",
    "        ('min_days_between_claims', 'min')\n",
    "    ]).reset_index()\n",
    "\n",
    "    claims_sorted['claim_order'] = claims_sorted.groupby(member_id_col).cumcount() + 1\n",
    "\n",
    "    def calculate_trend(group: pd.DataFrame) -> float:\n",
    "        if len(group) < 2:\n",
    "            return 0.0\n",
    "        x = group['claim_order'].values\n",
    "        y = group['claim_amount'].values\n",
    "        if len(x) > 1 and np.std(x) > 0:\n",
    "            corr = np.corrcoef(x, y)[0, 1]\n",
    "            return float(corr) if not np.isnan(corr) else 0.0\n",
    "        return 0.0\n",
    "\n",
    "    claim_trends = claims_sorted.groupby(member_id_col).apply(calculate_trend).reset_index()\n",
    "    claim_trends.columns = [member_id_col, 'claim_amount_trend']\n",
    "\n",
    "    last_claim = historical_claims.groupby(member_id_col)['paid_date'].max().reset_index()\n",
    "    last_claim.columns = [member_id_col, 'last_claim_date']\n",
    "    last_claim['days_since_last_claim'] = (observation_date - last_claim['last_claim_date']).dt.days\n",
    "    last_claim['months_since_last_claim'] = last_claim['days_since_last_claim'] / 30.44\n",
    "\n",
    "    first_claim = historical_claims.groupby(member_id_col)['paid_date'].min().reset_index()\n",
    "    first_claim.columns = [member_id_col, 'first_claim_date']\n",
    "    first_claim['days_since_first_claim'] = (observation_date - first_claim['first_claim_date']).dt.days\n",
    "\n",
    "    if 'condition_category' in historical_claims.columns:\n",
    "        condition_diversity = historical_claims.groupby(member_id_col)['condition_category'].nunique().reset_index()\n",
    "        condition_diversity.columns = [member_id_col, 'unique_conditions_count']\n",
    "        chronic_claims = historical_claims[historical_claims.get('is_chronic_condition', 0) == 1]\n",
    "        chronic_count = chronic_claims.groupby(member_id_col).size().reset_index(name='chronic_condition_claims')\n",
    "    else:\n",
    "        condition_diversity = pd.DataFrame({member_id_col: [], 'unique_conditions_count': []})\n",
    "        chronic_count = pd.DataFrame({member_id_col: [], 'chronic_condition_claims': []})\n",
    "\n",
    "    if 'treatment_type' in historical_claims.columns:\n",
    "        treatment_diversity = historical_claims.groupby(member_id_col)['treatment_type'].nunique().reset_index()\n",
    "        treatment_diversity.columns = [member_id_col, 'unique_treatments_count']\n",
    "    else:\n",
    "        treatment_diversity = pd.DataFrame({member_id_col: [], 'unique_treatments_count': []})\n",
    "\n",
    "    if 'provider_type' in historical_claims.columns:\n",
    "        provider_diversity = historical_claims.groupby(member_id_col)['provider_type'].nunique().reset_index()\n",
    "        provider_diversity.columns = [member_id_col, 'unique_providers_count']\n",
    "    else:\n",
    "        provider_diversity = pd.DataFrame({member_id_col: [], 'unique_providers_count': []})\n",
    "\n",
    "    if 'treatment_location' in historical_claims.columns:\n",
    "        location_diversity = historical_claims.groupby(member_id_col)['treatment_location'].nunique().reset_index()\n",
    "        location_diversity.columns = [member_id_col, 'unique_locations_count']\n",
    "    else:\n",
    "        location_diversity = pd.DataFrame({member_id_col: [], 'unique_locations_count': []})\n",
    "\n",
    "    if 'is_inpatient' in historical_claims.columns:\n",
    "        inpatient_stats = historical_claims.groupby(member_id_col).agg({\n",
    "            'is_inpatient': ['sum', 'mean']\n",
    "        }).reset_index()\n",
    "        inpatient_stats.columns = [member_id_col, 'total_inpatient_claims', 'inpatient_ratio']\n",
    "    else:\n",
    "        inpatient_stats = pd.DataFrame({member_id_col: [], 'total_inpatient_claims': [], 'inpatient_ratio': []})\n",
    "\n",
    "    if 'paid_in_winter' in historical_claims.columns:\n",
    "        seasonal_stats = historical_claims.groupby(member_id_col).agg({\n",
    "            'paid_in_winter': 'sum',\n",
    "            'paid_in_flu_season': 'sum'\n",
    "        }).reset_index()\n",
    "        seasonal_stats.columns = [member_id_col, 'winter_claims', 'flu_season_claims']\n",
    "    else:\n",
    "        seasonal_stats = pd.DataFrame({member_id_col: [], 'winter_claims': [], 'flu_season_claims': []})\n",
    "\n",
    "    member_agg['claim_amount_cv'] = member_agg['std_claim_amount'] / (member_agg['avg_claim_amount'] + 1)\n",
    "\n",
    "    member_features = members_df.copy()\n",
    "    merge_frames = [member_agg] + member_period_aggs + [\n",
    "        high_cost_agg, claim_frequency, claim_trends, last_claim, first_claim,\n",
    "        condition_diversity, chronic_count, treatment_diversity, provider_diversity,\n",
    "        location_diversity, inpatient_stats, seasonal_stats\n",
    "    ]\n",
    "    for frame in merge_frames:\n",
    "        if not frame.empty:\n",
    "            member_features = member_features.merge(frame, on=member_id_col, how='left')\n",
    "\n",
    "    count_columns = [\n",
    "        'total_claims', 'claim_amount_12m_count', 'claim_amount_6m_count', 'claim_amount_3m_count',\n",
    "        'claim_amount_high_cost_count', 'winter_claims', 'flu_season_claims',\n",
    "        'chronic_condition_claims', 'total_inpatient_claims', 'unique_conditions_count',\n",
    "        'unique_treatments_count', 'unique_providers_count', 'unique_locations_count'\n",
    "    ]\n",
    "    for col in count_columns:\n",
    "        if col in member_features.columns:\n",
    "            member_features[col] = member_features[col].fillna(0)\n",
    "\n",
    "    sum_columns = [\n",
    "        'total_claim_amount', 'claim_amount_12m_sum', 'claim_amount_6m_sum',\n",
    "        'claim_amount_3m_sum', 'claim_amount_high_cost_sum'\n",
    "    ]\n",
    "    for col in sum_columns:\n",
    "        if col in member_features.columns:\n",
    "            member_features[col] = member_features[col].fillna(0)\n",
    "\n",
    "    if 'inpatient_ratio' in member_features.columns:\n",
    "        member_features['inpatient_ratio'] = member_features['inpatient_ratio'].fillna(0)\n",
    "    if 'claim_amount_cv' in member_features.columns:\n",
    "        member_features['claim_amount_cv'] = member_features['claim_amount_cv'].fillna(0)\n",
    "\n",
    "    if {'claim_amount_3m_count', 'total_claims'}.issubset(member_features.columns):\n",
    "        member_features['claims_acceleration_3m'] = member_features['claim_amount_3m_count'] - (\n",
    "            member_features['total_claims'] / ((member_features['membership_tenure_years'] * 12) + 1)\n",
    "        )\n",
    "    if {'claim_amount_12m_count', 'membership_tenure_years'}.issubset(member_features.columns):\n",
    "        member_features['claims_acceleration_12m'] = member_features['claim_amount_12m_count'] - (\n",
    "            member_features['total_claims'] / (member_features['membership_tenure_years'] + 1)\n",
    "        )\n",
    "    if {'claim_amount_3m_sum', 'claim_amount_12m_sum'}.issubset(member_features.columns):\n",
    "        member_features['recent_activity_ratio'] = member_features['claim_amount_3m_sum'] / (\n",
    "            member_features['claim_amount_12m_sum'] + 1\n",
    "        )\n",
    "    if {'claim_amount_high_cost_sum', 'total_claim_amount'}.issubset(member_features.columns):\n",
    "        member_features['high_cost_ratio'] = member_features['claim_amount_high_cost_sum'] / (\n",
    "            member_features['total_claim_amount'] + 1\n",
    "        )\n",
    "    if {'total_claims', 'membership_tenure_years'}.issubset(member_features.columns):\n",
    "        member_features['claims_per_year_of_membership'] = member_features['total_claims'] / (\n",
    "            member_features['membership_tenure_years'] + 0.1\n",
    "        )\n",
    "    if {'avg_claim_amount', 'max_claim_amount'}.issubset(member_features.columns):\n",
    "        member_features['avg_to_max_ratio'] = member_features['avg_claim_amount'] / (\n",
    "            member_features['max_claim_amount'] + 1\n",
    "        )\n",
    "\n",
    "    def _column_or_zero(df: pd.DataFrame, col: str) -> pd.Series:\n",
    "        if col in df.columns:\n",
    "            return df[col].fillna(0)\n",
    "        return pd.Series(0, index=df.index, dtype=float)\n",
    "\n",
    "    member_features['has_claim_history'] = (_column_or_zero(member_features, 'total_claims') > 0).astype(int)\n",
    "    member_features['has_recent_claims'] = (_column_or_zero(member_features, 'claim_amount_3m_count') > 0).astype(int)\n",
    "    member_features['has_high_cost_history'] = (_column_or_zero(member_features, 'claim_amount_high_cost_count') > 0).astype(int)\n",
    "    member_features['has_chronic_history'] = (_column_or_zero(member_features, 'chronic_condition_claims') > 0).astype(int)\n",
    "\n",
    "    total_claims_series = _column_or_zero(member_features, 'total_claims')\n",
    "    member_features['is_frequent_claimer'] = (total_claims_series > total_claims_series.median()).astype(int)\n",
    "\n",
    "    member_features['risk_score'] = (\n",
    "        member_features['has_high_cost_history'] * 2 +\n",
    "        member_features['has_chronic_history'] +\n",
    "        member_features['is_frequent_claimer'] +\n",
    "        member_features['has_recent_claims']\n",
    "    )\n",
    "    return member_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2be1a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_feature_engineering_pipeline(\n",
    "    claims_df: pd.DataFrame,\n",
    "    members_df: pd.DataFrame,\n",
    "    observation_date: pd.Timestamp | None = None\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Run the same feature pipeline used during training.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"FEATURE ENGINEERING PIPELINE\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    if observation_date is None:\n",
    "        observation_date = pd.Timestamp.now()\n",
    "    print(f\"\\nObservation Date: {observation_date.date()}\")\n",
    "\n",
    "    print(\"\\n1. Creating member-level features...\")\n",
    "    members_featured = create_member_features(members_df)\n",
    "    print(f\"   Member features created. Shape: {members_featured.shape}\")\n",
    "\n",
    "    print(\"\\n2. Creating claim-level features...\")\n",
    "    claims_featured = create_claims_features(claims_df, reference_date=observation_date)\n",
    "    print(f\"   Claim features created. Shape: {claims_featured.shape}\")\n",
    "\n",
    "    print(\"\\n3. Creating aggregated member-claim features...\")\n",
    "    final_dataset = create_aggregated_member_claim_features(claims_featured, members_featured, observation_date)\n",
    "    print(f\"   Aggregated features created. Shape: {final_dataset.shape}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "    print(f\"Final Dataset Shape: {final_dataset.shape}\")\n",
    "    print(f\"Total Features: {len(final_dataset.columns)}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return final_dataset, claims_featured\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c1a3313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inference_features(\n",
    "    raw_claims: pd.DataFrame,\n",
    "    raw_members: pd.DataFrame,\n",
    "    observation_date: pd.Timestamp | None = None\n",
    ") -> tuple[pd.DataFrame, pd.Timestamp]:\n",
    "    \"\"\"Clean inputs and rebuild feature matrix for inference.\"\"\"\n",
    "    claims_clean = clean_claims_data(raw_claims)\n",
    "    members_clean = clean_membership_data(raw_members)\n",
    "\n",
    "    # Choose observation date as latest paid date if not provided\n",
    "    if observation_date is None:\n",
    "        if 'paid_date' in claims_clean.columns and claims_clean['paid_date'].notna().any():\n",
    "            observation_date = claims_clean['paid_date'].max()\n",
    "        else:\n",
    "            observation_date = pd.Timestamp.now()\n",
    "\n",
    "    final_memberset, _ = full_feature_engineering_pipeline(\n",
    "        claims_clean, members_clean, observation_date\n",
    "    )\n",
    "\n",
    "    if 'scheme_type' in final_memberset.columns:\n",
    "        final_memberset = final_memberset.drop(columns=['scheme_type'])\n",
    "\n",
    "    return final_memberset, observation_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e697b55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_26072\\968998591.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_clean['treatment_type'].fillna('Unknown', inplace=True)\n",
      "C:\\Temp\\ipykernel_26072\\968998591.py:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_clean['provider_type'].fillna('Unknown', inplace=True)\n",
      "C:\\Temp\\ipykernel_26072\\968998591.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_clean['treatment_location'].fillna(mode_val[0] if not mode_val.empty else 'Unknown', inplace=True)\n",
      "C:\\Temp\\ipykernel_26072\\968998591.py:34: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_clean['condition_category'].fillna('Unspecified', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claims Cleaning Summary -> original: 1,000, cleaned: 992\n",
      "Membership Cleaning Summary -> original: 873, cleaned: 873\n",
      "================================================================================\n",
      "FEATURE ENGINEERING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "Observation Date: 2025-11-02\n",
      "\n",
      "1. Creating member-level features...\n",
      "   Member features created. Shape: (873, 29)\n",
      "\n",
      "2. Creating claim-level features...\n",
      "   Claim features created. Shape: (992, 70)\n",
      "\n",
      "3. Creating aggregated member-claim features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_26072\\1216600732.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  claim_trends = claims_sorted.groupby(member_id_col).apply(calculate_trend).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Aggregated features created. Shape: (873, 75)\n",
      "\n",
      "================================================================================\n",
      "FEATURE ENGINEERING COMPLETE\n",
      "Final Dataset Shape: (873, 75)\n",
      "Total Features: 75\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumit\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference completed for 873 members as of 2025-11-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sumit\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claimant_unique_id</th>\n",
       "      <th>Predicted_Claim_Amount</th>\n",
       "      <th>High_Claim_Probability</th>\n",
       "      <th>Will_Have_High_Claim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MEM00014922-01</td>\n",
       "      <td>104851.774649</td>\n",
       "      <td>0.722488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MEM00004162-01</td>\n",
       "      <td>21911.361439</td>\n",
       "      <td>0.148439</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEM00003152-02</td>\n",
       "      <td>21327.256457</td>\n",
       "      <td>0.104796</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MEM00012511-02</td>\n",
       "      <td>21368.722743</td>\n",
       "      <td>0.088494</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MEM00048133-03</td>\n",
       "      <td>25964.177571</td>\n",
       "      <td>0.029278</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  claimant_unique_id  Predicted_Claim_Amount  High_Claim_Probability  \\\n",
       "0     MEM00014922-01           104851.774649                0.722488   \n",
       "1     MEM00004162-01            21911.361439                0.148439   \n",
       "2     MEM00003152-02            21327.256457                0.104796   \n",
       "3     MEM00012511-02            21368.722743                0.088494   \n",
       "4     MEM00048133-03            25964.177571                0.029278   \n",
       "\n",
       "   Will_Have_High_Claim  \n",
       "0                     1  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1. Ensure input dataframes are available ---------------------------------\n",
    "try:\n",
    "    claims_input = claims_sample.copy()\n",
    "except NameError as exc:\n",
    "    raise RuntimeError(\"Define 'claims_sample' before running inference.\") from exc\n",
    "\n",
    "try:\n",
    "    members_input = mem_combined.copy()\n",
    "except NameError:\n",
    "    try:\n",
    "        members_input = mem_combine.copy()\n",
    "    except NameError as exc:\n",
    "        raise RuntimeError(\"Define 'mem_combined' (or 'mem_combine') before running inference.\") from exc\n",
    "\n",
    "# --- 2. Rebuild features -------------------------------------------------------\n",
    "member_feature_df, as_of_date = prepare_inference_features(claims_input, members_input)\n",
    "member_id_col = 'claimant_unique_id' if 'claimant_unique_id' in member_feature_df.columns else 'unique_id'\n",
    "if member_id_col not in member_feature_df.columns:\n",
    "    raise ValueError(\"Could not identify member identifier column in engineered features.\")\n",
    "\n",
    "# --- 3. Load production artifacts ---------------------------------------------\n",
    "model_dir = 'production_models'\n",
    "if not os.path.isdir(model_dir):\n",
    "    raise FileNotFoundError(\"production_models directory not found. Run the training notebook export step first.\")\n",
    "\n",
    "paths = {\n",
    "    'regressor': os.path.join(model_dir, 'xgb_regressor.pkl'),\n",
    "    'classifier': os.path.join(model_dir, 'xgb_classifier.pkl'),\n",
    "    'multiclass': os.path.join(model_dir, 'xgb_multiclass.pkl'),\n",
    "    'scaler': os.path.join(model_dir, 'scaler.pkl'),\n",
    "    'encoders': os.path.join(model_dir, 'label_encoders.pkl'),\n",
    "    'risk_encoder': os.path.join(model_dir, 'risk_encoder.pkl'),\n",
    "    'feature_names': os.path.join(model_dir, 'feature_names.json'),\n",
    "    'metadata': os.path.join(model_dir, 'metadata.json')\n",
    "}\n",
    "\n",
    "missing = [name for name, path in paths.items() if name in ['regressor', 'classifier', 'feature_names'] and not os.path.exists(path)]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"Missing required artifact(s): {missing}. Re-run model export in the training notebook.\")\n",
    "\n",
    "reg_model = joblib.load(paths['regressor'])\n",
    "clf_model = joblib.load(paths['classifier'])\n",
    "multiclass_model = joblib.load(paths['multiclass']) if os.path.exists(paths['multiclass']) else None\n",
    "scaler = joblib.load(paths['scaler']) if os.path.exists(paths['scaler']) else None\n",
    "label_encoders = joblib.load(paths['encoders']) if os.path.exists(paths['encoders']) else {}\n",
    "risk_encoder = joblib.load(paths['risk_encoder']) if os.path.exists(paths['risk_encoder']) else None\n",
    "\n",
    "with open(paths['feature_names'], 'r') as f:\n",
    "    feature_names = json.load(f)\n",
    "metadata = {}\n",
    "if os.path.exists(paths['metadata']):\n",
    "    with open(paths['metadata'], 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "# --- 4. Align feature matrix ---------------------------------------------------\n",
    "features = member_feature_df.copy()\n",
    "for col in feature_names:\n",
    "    if col not in features.columns:\n",
    "        features[col] = 0\n",
    "\n",
    "missing_after = [col for col in feature_names if col not in features.columns]\n",
    "if missing_after:\n",
    "    raise ValueError(f\"Unable to construct required features: {missing_after}\")\n",
    "\n",
    "X = features[feature_names].copy()\n",
    "\n",
    "for col, encoder in label_encoders.items():\n",
    "    if col in X.columns:\n",
    "        mapping = {cls: idx for idx, cls in enumerate(encoder.classes_)}\n",
    "        X[col] = X[col].astype(str).map(mapping).fillna(-1).astype(int)\n",
    "\n",
    "X = X.replace([np.inf, -np.inf], np.nan).fillna(-999)\n",
    "X_matrix = scaler.transform(X) if scaler is not None else X.values\n",
    "\n",
    "# --- 5. Run predictions --------------------------------------------------------\n",
    "reg_predictions = reg_model.predict(X_matrix)\n",
    "clf_probabilities = clf_model.predict_proba(X_matrix)[:, 1]\n",
    "clf_flags = (clf_probabilities >= 0.5).astype(int)\n",
    "\n",
    "if multiclass_model is not None and risk_encoder is not None:\n",
    "    risk_labels = risk_encoder.inverse_transform(multiclass_model.predict(X_matrix))\n",
    "else:\n",
    "    risk_labels = None\n",
    "\n",
    "# --- 6. Assemble output -------------------------------------------------------\n",
    "results = pd.DataFrame({\n",
    "    member_id_col: member_feature_df[member_id_col],\n",
    "    'Predicted_Claim_Amount': reg_predictions,\n",
    "    'High_Claim_Probability': clf_probabilities,\n",
    "    'Will_Have_High_Claim': clf_flags\n",
    "})\n",
    "if risk_labels is not None:\n",
    "    results['Predicted_Risk_Category'] = risk_labels\n",
    "\n",
    "if 'high_claim_threshold' in metadata:\n",
    "    results.attrs['high_claim_threshold'] = metadata['high_claim_threshold']\n",
    "\n",
    "results_sorted = results.sort_values('High_Claim_Probability', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f\"Inference completed for {len(results_sorted):,} members as of {as_of_date.date()}.\")\n",
    "results_sorted.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "81cc31a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claimant_unique_id</th>\n",
       "      <th>Predicted_Claim_Amount</th>\n",
       "      <th>High_Claim_Probability</th>\n",
       "      <th>Will_Have_High_Claim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MEM00014922-01</td>\n",
       "      <td>104851.774649</td>\n",
       "      <td>0.722488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  claimant_unique_id  Predicted_Claim_Amount  High_Claim_Probability  \\\n",
       "0     MEM00014922-01           104851.774649                0.722488   \n",
       "\n",
       "   Will_Have_High_Claim  \n",
       "0                     1  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_sorted[results_sorted['Will_Have_High_Claim'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cd8a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e9ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: persist outputs for downstream use\n",
    "artifacts_dir = 'artifacts'\n",
    "os.makedirs(artifacts_dir, exist_ok=True)\n",
    "\n",
    "predictions_path = os.path.join(artifacts_dir, 'production_inference_predictions.csv')\n",
    "results_sorted.to_csv(predictions_path, index=False)\n",
    "print(f\"Saved member-level predictions to: {predictions_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09c43a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f9339df",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_df = pd.read_csv(\"uk_pmi_claims_200k.csv\")\n",
    "memebrship_df = pd.read_csv(\"uk_pmi_membership_120k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8268173",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_sample = claims_df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d0f1238",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set(claims_sample['Claimant Unique ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e5e49d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = memebrship_df[memebrship_df['Unique ID'].isin(a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b382aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = memebrship_df[~memebrship_df['Unique ID'].isin(a)].sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "321f2bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_combined = pd.concat([x,y], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "608addcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_combined = mem_combined.rename(columns = {'Unique ID':'claimant_unique_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c30dfc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Client Name</th>\n",
       "      <th>Client Identifier</th>\n",
       "      <th>Scheme Category/ Section Name</th>\n",
       "      <th>Scheme Category/ Section Name Identifier</th>\n",
       "      <th>Unique Member Reference</th>\n",
       "      <th>claimant_unique_id</th>\n",
       "      <th>Status of Member</th>\n",
       "      <th>Status of Registration</th>\n",
       "      <th>Year of Birth</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Short Post Code of Member</th>\n",
       "      <th>Contract Start Date</th>\n",
       "      <th>Contract End Date</th>\n",
       "      <th>Original Date of Joining</th>\n",
       "      <th>Scheme Policy Joining Date</th>\n",
       "      <th>Registration Status</th>\n",
       "      <th>Lapse Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>Medicash</td>\n",
       "      <td>CLI0014</td>\n",
       "      <td>Comprehensive Cover</td>\n",
       "      <td>SCH0002</td>\n",
       "      <td>MEM00018476</td>\n",
       "      <td>MEM00018476-02</td>\n",
       "      <td>Partner</td>\n",
       "      <td>Family</td>\n",
       "      <td>1994</td>\n",
       "      <td>Male</td>\n",
       "      <td>B16</td>\n",
       "      <td>2022-10-31</td>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>2020-10-15</td>\n",
       "      <td>2022-05-23</td>\n",
       "      <td>Active</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>Healix Health Services</td>\n",
       "      <td>CLI0009</td>\n",
       "      <td>Essential Cover</td>\n",
       "      <td>SCH0009</td>\n",
       "      <td>MEM00019592</td>\n",
       "      <td>MEM00019592-01</td>\n",
       "      <td>Member</td>\n",
       "      <td>Single</td>\n",
       "      <td>1961</td>\n",
       "      <td>Female</td>\n",
       "      <td>RG2</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>2024-01-15</td>\n",
       "      <td>2021-06-19</td>\n",
       "      <td>2022-08-14</td>\n",
       "      <td>Active</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>HSF Health Plan</td>\n",
       "      <td>CLI0012</td>\n",
       "      <td>Core Cover</td>\n",
       "      <td>SCH0001</td>\n",
       "      <td>MEM00033757</td>\n",
       "      <td>MEM00033757-01</td>\n",
       "      <td>Member</td>\n",
       "      <td>Family</td>\n",
       "      <td>1991</td>\n",
       "      <td>Female</td>\n",
       "      <td>CR0</td>\n",
       "      <td>2023-04-23</td>\n",
       "      <td>2024-04-22</td>\n",
       "      <td>2022-12-24</td>\n",
       "      <td>2022-06-14</td>\n",
       "      <td>Active</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Benenden Health</td>\n",
       "      <td>CLI0007</td>\n",
       "      <td>Elite Plan</td>\n",
       "      <td>SCH0012</td>\n",
       "      <td>MEM00005666</td>\n",
       "      <td>MEM00005666-01</td>\n",
       "      <td>Member</td>\n",
       "      <td>Family</td>\n",
       "      <td>1967</td>\n",
       "      <td>Male</td>\n",
       "      <td>M1</td>\n",
       "      <td>2020-12-11</td>\n",
       "      <td>2021-12-11</td>\n",
       "      <td>2018-10-03</td>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>Active</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>Westfield Health</td>\n",
       "      <td>CLI0013</td>\n",
       "      <td>Platinum Plan</td>\n",
       "      <td>SCH0008</td>\n",
       "      <td>MEM00052440</td>\n",
       "      <td>MEM00052440-03</td>\n",
       "      <td>Dependent</td>\n",
       "      <td>Couple</td>\n",
       "      <td>2005</td>\n",
       "      <td>Female</td>\n",
       "      <td>LS1</td>\n",
       "      <td>2023-10-25</td>\n",
       "      <td>2024-10-24</td>\n",
       "      <td>2021-11-27</td>\n",
       "      <td>2023-04-04</td>\n",
       "      <td>Active</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Client Name Client Identifier Scheme Category/ Section Name  \\\n",
       "268                Medicash           CLI0014           Comprehensive Cover   \n",
       "286  Healix Health Services           CLI0009               Essential Cover   \n",
       "497         HSF Health Plan           CLI0012                    Core Cover   \n",
       "78          Benenden Health           CLI0007                    Elite Plan   \n",
       "763        Westfield Health           CLI0013                 Platinum Plan   \n",
       "\n",
       "    Scheme Category/ Section Name Identifier Unique Member Reference  \\\n",
       "268                                  SCH0002             MEM00018476   \n",
       "286                                  SCH0009             MEM00019592   \n",
       "497                                  SCH0001             MEM00033757   \n",
       "78                                   SCH0012             MEM00005666   \n",
       "763                                  SCH0008             MEM00052440   \n",
       "\n",
       "    claimant_unique_id Status of Member Status of Registration  Year of Birth  \\\n",
       "268     MEM00018476-02          Partner                 Family           1994   \n",
       "286     MEM00019592-01           Member                 Single           1961   \n",
       "497     MEM00033757-01           Member                 Family           1991   \n",
       "78      MEM00005666-01           Member                 Family           1967   \n",
       "763     MEM00052440-03        Dependent                 Couple           2005   \n",
       "\n",
       "     Gender Short Post Code of Member Contract Start Date Contract End Date  \\\n",
       "268    Male                       B16          2022-10-31        2023-10-31   \n",
       "286  Female                       RG2          2023-01-15        2024-01-15   \n",
       "497  Female                       CR0          2023-04-23        2024-04-22   \n",
       "78     Male                        M1          2020-12-11        2021-12-11   \n",
       "763  Female                       LS1          2023-10-25        2024-10-24   \n",
       "\n",
       "    Original Date of Joining Scheme Policy Joining Date Registration Status  \\\n",
       "268               2020-10-15                 2022-05-23              Active   \n",
       "286               2021-06-19                 2022-08-14              Active   \n",
       "497               2022-12-24                 2022-06-14              Active   \n",
       "78                2018-10-03                 2020-01-08              Active   \n",
       "763               2021-11-27                 2023-04-04              Active   \n",
       "\n",
       "    Lapse Date  \n",
       "268        NaN  \n",
       "286        NaN  \n",
       "497        NaN  \n",
       "78         NaN  \n",
       "763        NaN  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_combined.sample(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
